{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "from gluonnlp.base import get_repo_url\n",
    "from gluonnlp.models.t5 import _build_t5_tokenizer\n",
    "from gluonnlp.utils.misc import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792k/792k [00:00<00:00, 17.0MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /tmp/tmp2gil57ci/t5_spm.model from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as dir_path: \n",
    "    vocab_path = os.path.join(dir_path, 't5_spm.model')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model',\n",
    "        path=vocab_path\n",
    "    )\n",
    "    tokenizer = _build_t5_tokenizer(vocab_path, False, 100)\n",
    "    os.remove(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import mxnet as mx\n",
    "from mxnet import np, npx\n",
    "import numpy as _np\n",
    "from transformers import T5Model as HFT5\n",
    "from transformers import T5Config as HFT5CFG\n",
    "from gluonnlp.models.t5 import T5Model as GLT5\n",
    "from gluonnlp.models.t5 import google_t5_small as GLT5CFG\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "npx.random.seed(0)\n",
    "_np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hft5 = HFT5(HFT5CFG())\n",
    "hft5.eval()\n",
    "\n",
    "glt5 = GLT5.from_cfg(GLT5CFG())\n",
    "# glt5.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft5.encoder.block[0].layer[0].SelfAttention.q.weight\n",
    "# glt5.encoder.layers[0].self_attn_q.weight.data()\n",
    "\n",
    "# d = {}\n",
    "# for (k, v) in hft5.state_dict().items(): \n",
    "#     d[k] = v.shape\n",
    "# d\n",
    "\n",
    "# glt5.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_MAP = [\n",
    "    # 0.\n",
    "    ('shared.weight', 'input_embedding_layer.weight'), \n",
    "    # 1. encoder / decoder\n",
    "    ('{}.block.0.layer.0.SelfAttention.relative_attention_bias.weight', '{}.relative_position_encoder._rel_pos_embed.weight'), \n",
    "    # 2. encoder / decoder, block/layer #, layer_norm->self_attn_layer_norm / SelfAttention.o->self_attn_proj\n",
    "    ('{}.block.{}.layer.0.{}.weight', '{}.layers.{}.{}.weight'), \n",
    "    # 3. encoder / decoder, block/layer #, 0.Self->self / 1.EncDec->cross, q/k/v\n",
    "    ('{}.block.{}.layer.{}Attention.{}.weight', '{}.layers.{}.{}_attn_{}.weight'), \n",
    "    # 4. block/layer #, layer_norm->cross_attn_layer_norm / EncDecAttention.o->cross_attn_proj\n",
    "    ('decoder.block.{}.layer.1.{}.weight', 'decoder.layers.{}.{}.weight'), \n",
    "    # 5. encoder / decoder, block/layer #, (encoder: 1 / decoder: 2), DenseReluDense.wi/wi_0/wi_1/wo / layer_norm\n",
    "    ('{}.block.{}.layer.{}.{}.weight', '{}.layers.{}.ffn.{}.weight'), \n",
    "    # 6. encoder / decoder\n",
    "    ('{}.final_layer_norm.weight', '{}.final_layer_norm.weight'), \n",
    "]\n",
    "\n",
    "def convert_params(hf_t5_model, gluon_t5_model, ctx): \n",
    "    gluon_t5_model.initialize(ctx=ctx)\n",
    "    hf_params = hf_t5_model.state_dict()\n",
    "    gluon_params = gluon_t5_model.collect_params()\n",
    "    # TODO(yongyi-wu): add sanity check, eg. param #, layer #, ffn activation, etc.\n",
    "    num_layers = gluon_t5_model.num_layers\n",
    "\n",
    "    def convert(hf_param, gluon_param): \n",
    "        gluon_params[gluon_param].set_data(\n",
    "            hf_params[hf_param].cpu().numpy()\n",
    "        )\n",
    "        \n",
    "    for idx, (hf_key, gluon_key) in enumerate(PARAM_MAP): \n",
    "        if idx == 0: \n",
    "            convert(hf_key, gluon_key)\n",
    "        elif idx == 1: \n",
    "            for i in ['encoder', 'decoder']: \n",
    "                convert(hf_key.format(i), gluon_key.format(i))\n",
    "        elif idx in [2, 3]: \n",
    "            for stack in ['encoder', 'decoder']: \n",
    "                for layer in range(num_layers): \n",
    "                    if 'Attention' not in hf_key: \n",
    "                        for i, j in [('layer_norm', 'self_attn_layer_norm'), ('SelfAttention.o', 'self_attn_proj')]: \n",
    "                            convert(hf_key.format(stack, layer, i), gluon_key.format(stack, layer, j))\n",
    "                    else: \n",
    "                        for i in ['q', 'k', 'v']: \n",
    "                            convert(hf_key.format(stack, layer, '0.Self', i), gluon_key.format(stack, layer, 'self', i))\n",
    "                            if stack == 'decoder': \n",
    "                                convert(hf_key.format(stack, layer, '1.EncDec', i), gluon_key.format(stack, layer, 'cross', i))\n",
    "        elif idx == 4:  \n",
    "            for layer in range(num_layers): \n",
    "                for i, j in [('layer_norm', 'cross_attn_layer_norm'), ('EncDecAttention.o', 'cross_attn_proj')]: \n",
    "                    convert(hf_key.format(layer, i), gluon_key.format(layer, j))\n",
    "        elif idx == 5:\n",
    "            for stack, i in [('encoder', 1), ('decoder', 2)]: \n",
    "                for layer in range(num_layers): \n",
    "                    if gluon_t5_model.activation == 'relu': \n",
    "                        denses = ['wi', 'wo']\n",
    "                    elif gluon_t5_model.activation == 'gated-gelu': \n",
    "                        denses = ['wi_0', 'wi_1', 'wo']\n",
    "                    else: \n",
    "                        raise ValueError('Unrecognized feed froward activation')\n",
    "                    for j in denses + ['layer_norm']: \n",
    "                        convert(\n",
    "                            hf_key.format(stack, layer, i, j if j == 'layer_norm' else 'DenseReluDense.{}'.format(j)), \n",
    "                            gluon_key.format(stack, layer, j)\n",
    "                        )\n",
    "        elif idx == 6: \n",
    "            for stack in ['encoder', 'decoder']: \n",
    "                convert(hf_key.format(stack), hf_key.format(stack))\n",
    "    \n",
    "    return gluon_t5_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glt5 = convert_params(hft5, glt5, mx.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:np.allclose is a fallback operator, which is actually using official numpy's implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for src, tgt in [\n",
    "    ('What time is it?', 'It is 2:00 PM.'), \n",
    "    ('Hello World?', 'World Hello!'), \n",
    "    ('These dead shall not have died in vain.', 'Government of the people, by the people, for the people shall not perish from the earth')\n",
    "]: \n",
    "    src_data = tokenizer.encode(src, int)\n",
    "    tgt_data = tokenizer.encode(tgt, int)\n",
    "\n",
    "    hf_src_data = torch.LongTensor([src_data])\n",
    "    hf_tgt_data = torch.LongTensor([tgt_data])\n",
    "    gl_src_data = np.array([src_data], dtype=np.int64)\n",
    "    gl_tgt_data = np.array([tgt_data], dtype=np.int64)\n",
    "\n",
    "    hf_res = hft5(input_ids=hf_src_data, decoder_input_ids=hf_tgt_data)['last_hidden_state'].detach().numpy()\n",
    "    gl_res = glt5(gl_src_data, np.array([len(gl_src_data[0])]), gl_tgt_data, np.array([len(gl_tgt_data[0])]))\n",
    "    \n",
    "    assert np.allclose(hf_res, gl_res, rtol=1e-05, atol=1e-05), \\\n",
    "        print('Transformer: {}\\nGluon-nlp: {}'.format(hf_res, gl_res))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
