{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import mxnet as mx\n",
    "from mxnet import np, npx\n",
    "from gluonnlp.attention_cell import gen_self_attn_mask, gen_mem_attn_mask\n",
    "from gluonnlp.data.tokenizers import SentencepieceTokenizer\n",
    "from gluonnlp.models.t5 import T5Model as Gluon_T5\n",
    "from gluonnlp.utils.misc import download, logging_config, sha1sum, naming_convention\n",
    "from transformers import T5Model as HF_T5\n",
    "\n",
    "\n",
    "# these mappings are adapted from huggingface T5 folder\n",
    "T5_PRETRAINED_MODEL_MAP = {\n",
    "    \"t5-small\": \"google_t5_small\",\n",
    "    \"t5-base\": \"google_t5_base\",\n",
    "    \"t5-large\": \"google_t5_large\",\n",
    "    \"t5-3b\": \"google_t5_3B\",\n",
    "    \"t5-11b\": \"google_t5_11B\"\n",
    "}\n",
    "T5_PRETRAINED_CONFIG_MAP = {\n",
    "    \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n",
    "    \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n",
    "    \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n",
    "    \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n",
    "    \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\"\n",
    "}\n",
    "PRETRAINED_VOCAB_MAP = {\n",
    "    \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n",
    "    \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n",
    "    \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n",
    "    \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n",
    "    \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\"\n",
    "}\n",
    "\n",
    "\n",
    "# this mapping only works on T5Model class from Huggingface and GluonNLP\n",
    "PARAM_MAP = [\n",
    "    # 0.\n",
    "    ('shared.weight', 'input_embedding_layer.weight'), \n",
    "    # 1. encoder / decoder\n",
    "    ('{}.block.0.layer.0.SelfAttention.relative_attention_bias.weight', '{}.relative_position_encoder._rel_pos_embed.weight'), \n",
    "    # 2. encoder / decoder, block/layer #, layer_norm->self_attn_layer_norm / SelfAttention.o->self_attn_proj\n",
    "    ('{}.block.{}.layer.0.{}.weight', '{}.layers.{}.{}.weight'), \n",
    "    # 3. encoder / decoder, block/layer #, 0.Self->self / 1.EncDec->cross, q/k/v\n",
    "    ('{}.block.{}.layer.{}Attention.{}.weight', '{}.layers.{}.{}_attn_{}.weight'), \n",
    "    # 4. block/layer #, layer_norm->cross_attn_layer_norm / EncDecAttention.o->cross_attn_proj\n",
    "    ('decoder.block.{}.layer.1.{}.weight', 'decoder.layers.{}.{}.weight'), \n",
    "    # 5. encoder / decoder, block/layer #, (encoder: 1 / decoder: 2), DenseReluDense.wi/wi_0/wi_1/wo / layer_norm\n",
    "    ('{}.block.{}.layer.{}.{}.weight', '{}.layers.{}.ffn.{}.weight'), \n",
    "    # 6. encoder / decoder\n",
    "    ('{}.final_layer_norm.weight', '{}.final_layer_norm.weight'), \n",
    "]\n",
    "\n",
    "\n",
    "def parse_args(): \n",
    "    parser = argparse.ArgumentParser('Convert Huggingface T5 Model to GluonNLP')\n",
    "    parser.add_argument(\n",
    "        'model_name', choices=list(T5_PRETRAINED_MODEL_MAP.keys()), help='Name of pretrained T5 model in Huggingface.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'dest_dir', help='Directory to save converted config, vocab and weights.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test', action='store_true', required=False, default=False, help='Whether to test conversion correctness.'\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def convert_config(args, converted): \n",
    "    print('converting cfg...')\n",
    "    # download config\n",
    "    gluon_cfg = Gluon_T5.get_cfg(T5_PRETRAINED_MODEL_MAP[args.model_name])\n",
    "    with tempfile.TemporaryDirectory() as temp_dir: \n",
    "        hf_cfg_path = os.path.join(temp_dir, 'config.json')\n",
    "        download(\n",
    "            url=T5_PRETRAINED_CONFIG_MAP[args.model_name], \n",
    "            path=hf_cfg_path\n",
    "        )\n",
    "        with open(hf_cfg_path, 'r') as f: \n",
    "            hf_cfg = json.load(f)\n",
    "        os.remove(hf_cfg_path)\n",
    "    # update attributes\n",
    "    cfg = gluon_cfg.clone()\n",
    "    cfg.defrost()\n",
    "    cfg.MODEL.vocab_size = hf_cfg['vocab_size']\n",
    "    cfg.MODEL.d_model = hf_cfg['d_model']\n",
    "    cfg.MODEL.d_kv = hf_cfg['d_kv']\n",
    "    cfg.MODEL.d_ff = hf_cfg['d_ff']\n",
    "    cfg.MODEL.num_layers = hf_cfg['num_layers']\n",
    "    cfg.MODEL.num_heads = hf_cfg['num_heads']\n",
    "    cfg.MODEL.layer_norm_eps = hf_cfg['layer_norm_epsilon']\n",
    "    cfg.MODEL.dropout_prob = hf_cfg['dropout_rate']\n",
    "    cfg.INITIALIZER.init_factor = hf_cfg['initializer_factor']\n",
    "    cfg.freeze()\n",
    "    # save config\n",
    "    config_path = os.path.join(args.dest_dir, '{}.yml'.format(\n",
    "        T5_PRETRAINED_MODEL_MAP[args.model_name]\n",
    "    ))\n",
    "    with open(config_path, 'w') as f: \n",
    "        f.write(cfg.dump())\n",
    "    converted['config'] = config_path\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def convert_vocab(args, converted): \n",
    "    print('converting vocab...')\n",
    "    # at this step we don't add <extra_id>s into the vocab, but just save the original binary file directly\n",
    "    # those pecial tokens are added only when T5 tokenizer gets instantiated from build_t5_tokenizer()\n",
    "    vocab_path = os.path.join(args.dest_dir, '{}.vocab'.format(\n",
    "        T5_PRETRAINED_MODEL_MAP[args.model_name]\n",
    "    ))\n",
    "    download(\n",
    "        url=PRETRAINED_VOCAB_MAP[args.model_name], \n",
    "        path=vocab_path\n",
    "    )\n",
    "    converted['vocab'] = vocab_path\n",
    "\n",
    "\n",
    "def convert_params(args, converted, hf_model, gluon_model): \n",
    "    print('converting parameters...')\n",
    "    # prepare models and parameters\n",
    "    gluon_model.initialize()\n",
    "    hf_params = hf_model.state_dict()\n",
    "    gluon_params = gluon_model.collect_params()\n",
    "    # TODO(yongyi-wu): add sanity check, eg. param #, layer #, ffn activation, etc.\n",
    "    num_layers = gluon_model.num_layers\n",
    "\n",
    "    def convert(hf_param, gluon_param): \n",
    "        gluon_params[gluon_param].set_data(hf_params[hf_param].cpu().numpy())\n",
    "    \n",
    "    # convert parameters\n",
    "    for idx, (hf_key, gluon_key) in enumerate(PARAM_MAP): \n",
    "        if idx == 0: \n",
    "            convert(hf_key, gluon_key)\n",
    "        elif idx == 1: \n",
    "            for i in ['encoder', 'decoder']: \n",
    "                convert(hf_key.format(i), gluon_key.format(i))\n",
    "        elif idx in [2, 3]: \n",
    "            for stack in ['encoder', 'decoder']: \n",
    "                for layer in range(num_layers): \n",
    "                    if 'Attention' not in hf_key: \n",
    "                        for i, j in [\n",
    "                            ('layer_norm', 'self_attn_layer_norm'), \n",
    "                            ('SelfAttention.o', 'self_attn_proj')\n",
    "                        ]: \n",
    "                            convert(\n",
    "                                hf_key.format(stack, layer, i), \n",
    "                                gluon_key.format(stack, layer, j)\n",
    "                            )\n",
    "                    else: \n",
    "                        for i in ['q', 'k', 'v']: \n",
    "                            convert(\n",
    "                                hf_key.format(stack, layer, '0.Self', i), \n",
    "                                gluon_key.format(stack, layer, 'self', i)\n",
    "                            )\n",
    "                            if stack == 'decoder': \n",
    "                                convert(\n",
    "                                    hf_key.format(stack, layer, '1.EncDec', i), \n",
    "                                    gluon_key.format(stack, layer, 'cross', i)\n",
    "                                )\n",
    "        elif idx == 4:  \n",
    "            for layer in range(num_layers): \n",
    "                for i, j in [\n",
    "                    ('layer_norm', 'cross_attn_layer_norm'), \n",
    "                    ('EncDecAttention.o', 'cross_attn_proj')\n",
    "                ]: \n",
    "                    convert(hf_key.format(layer, i), gluon_key.format(layer, j))\n",
    "        elif idx == 5:\n",
    "            for stack, i in [('encoder', 1), ('decoder', 2)]: \n",
    "                for layer in range(num_layers): \n",
    "                    if gluon_model.activation == 'relu': \n",
    "                        denses = ['wi', 'wo']\n",
    "                    elif gluon_model.activation == 'gated-gelu': \n",
    "                        denses = ['wi_0', 'wi_1', 'wo']\n",
    "                    else: \n",
    "                        raise ValueError('Unrecognized feed froward activation')\n",
    "                    for j in denses + ['layer_norm']: \n",
    "                        convert(\n",
    "                            hf_key.format(stack, layer, i, j if j == 'layer_norm' else 'DenseReluDense.{}'.format(j)), \n",
    "                            gluon_key.format(stack, layer, j)\n",
    "                        )\n",
    "        elif idx == 6: \n",
    "            for stack in ['encoder', 'decoder']: \n",
    "                convert(hf_key.format(stack), hf_key.format(stack))\n",
    "    # save parameters\n",
    "    param_path = os.path.join(args.dest_dir, '{}.params'.format(\n",
    "        T5_PRETRAINED_MODEL_MAP[args.model_name]\n",
    "    ))\n",
    "    gluon_model.save_parameters(param_path)\n",
    "    converted['params'] = param_path\n",
    "    return gluon_model\n",
    "\n",
    "\n",
    "def rename(args, converted): \n",
    "    for item, old_path in converted.items(): \n",
    "        new_name, _ = naming_convention(args.dest_dir, os.path.basename(old_path))\n",
    "        new_path = os.path.join(args.dest_dir, new_name)\n",
    "        shutil.move(old_path, new_path)\n",
    "        logging.info('{} of {} has been converted to {}.'.format(item, args.model_name, new_path))\n",
    "        converted[item] = new_path\n",
    "\n",
    "\n",
    "def convert_t5(args): \n",
    "    logging.info('converting T5 model from Huggingface...')\n",
    "    if not os.path.exists(args.dest_dir): \n",
    "        os.mkdir(args.dest_dir)\n",
    "    converted = {}\n",
    "    # convert and save vocab\n",
    "    convert_vocab(args, converted)\n",
    "    # convert and save config\n",
    "    gluon_cfg = convert_config(args, converted)\n",
    "    # convert, (test), and save model\n",
    "    hf_t5 = HF_T5.from_pretrained(args.model_name)\n",
    "    gluon_t5 = Gluon_T5.from_cfg(gluon_cfg)\n",
    "    gluon_t5 = convert_params(args, converted, hf_t5, gluon_t5)\n",
    "    # test model if needed\n",
    "    if args.test: \n",
    "        test_conversion(hf_t5, gluon_t5)\n",
    "    # rename with sha1sum\n",
    "    rename(args, converted)\n",
    "    logging.info('conversion completed.')\n",
    "    logging.info('file statistics:')\n",
    "    for item, new_path in converted.items(): \n",
    "        logging.info('filename: {}\\tsize: {}\\tsha1sum: {}'.format(\n",
    "            os.path.basename(new_path), os.path.getsize(new_path), sha1sum(new_path)\n",
    "        ))\n",
    "    return hf_t5, gluon_t5, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object): \n",
    "    def __init__(self): \n",
    "        self.model_name = 't5-base'\n",
    "        self.dest_dir = '/home/ubuntu/yongyiw/temp'\n",
    "        self.test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting vocab...\n",
      "Downloading /home/ubuntu/yongyiw/temp/google_t5_base.vocab from https://huggingface.co/t5-base/resolve/main/spiece.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792k/792k [00:00<00:00, 1.91MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting cfg...\n",
      "Downloading /tmp/tmprn7thnjf/config.json from https://huggingface.co/t5-base/resolve/main/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.20k/1.20k [00:00<00:00, 831kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting parameters...\n"
     ]
    }
   ],
   "source": [
    "args = A()\n",
    "hf_model, gluon_model = convert_t5(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[[ True  True False False]\n",
      " [ True  True  True False]]\n",
      "\n",
      "[1 2]\n",
      "[[ True False False]\n",
      " [ True  True False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# print('testing model...')\n",
    "# create dummy input\n",
    "batch_size = 2\n",
    "src_length = 4\n",
    "tgt_length = 3\n",
    "vocab_size = hf_model.shared.weight.shape[0]\n",
    "src_data = np.random.randint(1, vocab_size, (batch_size, src_length))\n",
    "src_valid_length = np.random.randint(src_length // 2, src_length, (batch_size,))\n",
    "# src_valid_length = np.full((batch_size,), src_length)\n",
    "tgt_data = np.random.randint(1, vocab_size, (batch_size, tgt_length))\n",
    "tgt_valid_length = np.random.randint(tgt_length // 2, tgt_length, (batch_size,))\n",
    "# tgt_valid_length = np.full((batch_size,), tgt_length)\n",
    "\n",
    "enc_attn_mask = npx.arange_like(src_data, axis=-1) < src_valid_length.reshape(-1, 1)\n",
    "dec_attn_mask = npx.arange_like(tgt_data, axis=-1) < tgt_valid_length.reshape(-1, 1)\n",
    "# enc_attn_mask = gen_self_attn_mask(\n",
    "#     np.expand_dims(src_data, axis=-1), \n",
    "#     src_valid_length, \n",
    "#     attn_type='full'\n",
    "# )\n",
    "# dec_attn_mask = gen_self_attn_mask(\n",
    "#     np.expand_dims(tgt_data, axis=-1), \n",
    "#     tgt_valid_length, \n",
    "#     attn_type='causal'\n",
    "# )\n",
    "print(src_valid_length)\n",
    "print(enc_attn_mask)\n",
    "print()\n",
    "print(tgt_valid_length)\n",
    "print(dec_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################\n",
      "GLUONNLP ENCODER\n",
      "self_attn_mask:\n",
      " (2, 4, 4)\n",
      "[[[ True  True False False]\n",
      "  [ True  True False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[ True  True  True False]\n",
      "  [ True  True  True False]\n",
      "  [ True  True  True False]\n",
      "  [False False False False]]]\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "GLUONNLP DECODER\n",
      "self_attn_mask:\n",
      " (2, 3, 3)\n",
      "[[[ True False False]\n",
      "  [False False False]\n",
      "  [False False False]]\n",
      "\n",
      " [[ True False False]\n",
      "  [ True  True False]\n",
      "  [False False False]]]\n",
      "\n",
      "cross_attn_mask:\n",
      " (2, 3, 4)\n",
      "[[[ True  True False False]\n",
      "  [False False False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[ True  True  True False]\n",
      "  [ True  True  True False]\n",
      "  [False False False False]]]\n",
      "\n",
      "\n",
      "###################\n",
      "HUGGINGFACE ENCODER\n",
      "self_attn_mask:\n",
      " torch.Size([2, 1, 1, 4])\n",
      "tensor([[[[    -0.,     -0., -10000., -10000.]]],\n",
      "\n",
      "\n",
      "        [[[    -0.,     -0.,     -0., -10000.]]]])\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "HUGGINGFACE DECODER\n",
      "self_attn_mask:\n",
      " torch.Size([2, 1, 3, 3])\n",
      "tensor([[[[    -0., -10000., -10000.],\n",
      "          [    -0., -10000., -10000.],\n",
      "          [    -0., -10000., -10000.]]],\n",
      "\n",
      "\n",
      "        [[[    -0., -10000., -10000.],\n",
      "          [    -0.,     -0., -10000.],\n",
      "          [    -0.,     -0., -10000.]]]])\n",
      "\n",
      "cross_attn_mask:\n",
      " torch.Size([2, 1, 1, 4])\n",
      "tensor([[[[    -0.,     -0., -10000., -10000.]]],\n",
      "\n",
      "\n",
      "        [[[    -0.,     -0.,     -0., -10000.]]]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:np.allclose is a fallback operator, which is actually using official numpy's implementation.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1430630c2edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m )['last_hidden_state'].detach().numpy()\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1E-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1E-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('###################')\n",
    "gl_out = gluon_model(src_data, src_valid_length, tgt_data, tgt_valid_length)\n",
    "\n",
    "print('###################')\n",
    "hf_model.eval() # disable dropout and test forward pass\n",
    "hf_out = hf_model(\n",
    "    input_ids=torch.from_numpy(src_data.asnumpy()),  \n",
    "    attention_mask=torch.from_numpy(enc_attn_mask.asnumpy()), \n",
    "    decoder_input_ids=torch.from_numpy(tgt_data.asnumpy()), \n",
    "    decoder_attention_mask=torch.from_numpy(dec_attn_mask.asnumpy())\n",
    ")['last_hidden_state'].detach().numpy()\n",
    "\n",
    "assert np.allclose(hf_out, gl_out, 1E-3, 1E-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
