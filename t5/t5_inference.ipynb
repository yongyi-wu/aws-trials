{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using T5 for Masked Language Modeling (MLM) Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in T5 model's paper[1], the author leveraged a pre-training technique called \"random spans\". As the name suggests, this method corrupts the input sequence by spans of tokens rather than individual tokens. After that, every noise spans are mapped to unique sentinels starting from `<extra_id_0>` and the objective is to denoise these spans. By handling consequentive corrupted token spans altogether, this method yields significant speed-up as compared to BERT's objective while retain performance. \n",
    "\n",
    "For example, suppose we have original sentence as \"Thank you for inviting me to your party last week\". After applying random spans, we may get \"Thank you for \\<extra_id_0\\> to \\<extra_id_1\\> week\", where the target is \"\\<extra_id_0\\> for inviting me \\<extra_id_1\\> your party last \\<extra_id_2\\>\". \n",
    "\n",
    "In this tutorial, we are going to: \n",
    "1. Load a pretrained T5 model\n",
    "2. Perform the MLM task on some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we load required packages. We also set seeds so that you could replicate our results on your machine. In addition, we introduce two helper functions for creating desired corrupted input and converting human-readable output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as _np\n",
    "from mxnet import np, npx\n",
    "from gluonnlp.data.batchify import Pad\n",
    "from gluonnlp.models import get_backbone\n",
    "from gluonnlp.models.t5 import T5Seq2seq, mask_to_sentinel\n",
    "from gluonnlp.sequence_sampler import BeamSearchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.set_np()\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "npx.random.seed(0)\n",
    "_np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_to_masks(tokenizer, tokens, spans): \n",
    "    def _spans_to_masks(tokens, spans): \n",
    "        if isinstance(tokens[0], int): \n",
    "            masks = [0] * len(tokens)\n",
    "            for i, span in enumerate(spans): \n",
    "                target = []\n",
    "                for idx in span: \n",
    "                    assert 0 <= idx and idx < len(tokens), 'Span index out of range'\n",
    "                    masks[idx] = 1\n",
    "                    target.append(tokens[idx])\n",
    "                print('{}: {}'.format(tokenizer.vocab.to_tokens(-1 - i), tokenizer.decode(target)), end='\\t')\n",
    "            print()\n",
    "            return masks\n",
    "        elif isinstance(tokens[0], list): \n",
    "            assert len(tokens) == len(spans), 'Every sample must have corresponding tokens and spans'\n",
    "            res = []\n",
    "            for i, (tok, s) in enumerate(zip(tokens, spans)): \n",
    "                print('[Sample {}]'.format(i), end='\\t')\n",
    "                res.append(_spans_to_masks(tok, s))\n",
    "            return res\n",
    "        else: \n",
    "            raise TypeError('Unsupported type of tokens: {}'.format(type(tokens)))\n",
    "    return _spans_to_masks(tokens, spans)\n",
    "\n",
    "\n",
    "def print_inference(tokenizer, output, spans): \n",
    "    for sample in range(output[0].shape[0]): \n",
    "        print('[SAMPLE {}]'.format(sample))\n",
    "        n_spans = len(spans[sample])\n",
    "        for beam in range(output[0].shape[1]): \n",
    "            print('(Beam {})'.format(beam), end='\\t')\n",
    "            ele_output = output[0][sample, beam, :enc_valid_length[sample].item()]\n",
    "            for n in range(n_spans): \n",
    "                i = np.where(ele_output == len(tokenizer.vocab) - 1 - n)[0][0].item()\n",
    "                j = i + 1 + len(spans[sample][n])\n",
    "                ele_tokens = ele_output[i + 1:j].tolist()\n",
    "                print('<extra_id_{}>: {}'.format(n, tokenizer.decode(ele_tokens)), end='\\t')\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_backbone` is a handy way to load models and download pretrained weights (not limited to T5) from the GluonNLP repository. Here, we choose T5-large for illustration purpose. Alternatively, you can use `google_t5_small`, `google_t5_base`, `google_t5_3B`, `google_t5_11B` as well. \n",
    "\n",
    "`T5Seq2seq` is a inference model equipped with the incremental decoding feature. Notice that it must be initialized with a `T5Model` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T5Model, cfg, tokenizer, local_params_path, _ = get_backbone('google_t5_large')\n",
    "backbone = T5Model.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "t5mlm = T5Seq2seq(backbone)\n",
    "t5mlm.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this MLM task, we will also leverage `BeamSearchSampler`, a powerful and easy-to-use tools in many scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 4\n",
    "t5mlm_seacher = BeamSearchSampler(beam_size, t5mlm, eos_id=1, vocab_size=32128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of our tokenizer is a Python list (or nested Python list), each sample can be of different lengths. This allows more flexibilities in manipulating intermediate results, but requires an additional step before feeding into the model. `Pad` helps us group multiple samples an ndarray batch in a clean way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = Pad(val=0, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we simply use a minibatch of two samples. We can inspect the tokenization result by passing `str` as the second argument. Notice that the tokenizer itself does not add EOS tokens, `</s>`, to the end of sequences. We leave the flexibility and responsibility to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    'Andrew Carnegie famously said , \" My heart is in the work . \" At CMU , we think about work a little differently .', \n",
    "    'Peace is a concept of societal friendship and harmony in the absence of hostility and violence . In a social sense , peace is commonly used to mean a lack of conflict and freedom from fear of violence between individuals or groups .' \n",
    "]\n",
    "tokens = tokenizer.encode(text, int)\n",
    "for ele_tokens in tokens: \n",
    "    ele_tokens.append(1) # append EOS token: </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Andrew', '▁Carnegie', '▁famous', 'ly', '▁said', '▁', ',', '▁\"', '▁My', '▁heart', '▁is', '▁in', '▁the', '▁work', '▁', '.', '▁\"', '▁At', '▁C', 'MU', '▁', ',', '▁we', '▁think', '▁about', '▁work', '▁', 'a', '▁little', '▁differently', '▁', '.']\n",
      "['▁Peace', '▁is', '▁', 'a', '▁concept', '▁of', '▁', 'societal', '▁friendship', '▁and', '▁harmony', '▁in', '▁the', '▁absence', '▁of', '▁host', 'ility', '▁and', '▁violence', '▁', '.', '▁In', '▁', 'a', '▁social', '▁sense', '▁', ',', '▁peace', '▁is', '▁commonly', '▁used', '▁to', '▁mean', '▁', 'a', '▁lack', '▁of', '▁conflict', '▁and', '▁freedom', '▁from', '▁fear', '▁of', '▁violence', '▁between', '▁individuals', '▁or', '▁groups', '▁', '.']\n"
     ]
    }
   ],
   "source": [
    "for ele_tokens in tokenizer.encode(text, str): \n",
    "    print(ele_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purpose, we manually define noise spans, although technically this should be a random process. Notice that every noise span corresponds to a tuple of token indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample 0]\t<extra_id_0>: in the work\t\n",
      "[Sample 1]\t<extra_id_0>: concept of \t<extra_id_1>: peace is commonly\t<extra_id_2>: individuals or groups\t\n"
     ]
    }
   ],
   "source": [
    "noise_spans = [\n",
    "    [(11, 12, 13)], # sequence 1\n",
    "    [(4, 5, 6), (28, 29, 30), (46, 47, 48)] # sequence 2\n",
    "]\n",
    "masks = spans_to_masks(tokenizer, tokens, noise_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted the `mask_to_sentinel()` from the `noise_span_to_unique_sentinel()` in the original [T5 repository](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py), which helps map noise spans to `<extra_id>` sentinels and collapse a span's tokens into a single sentinel. For the input, `tokens` and `masks` are required to have the exact shape. \n",
    "> For curious readers, there are many more useful implementations in T5's original repository. \n",
    "\n",
    "The preparation step completes as we batch-ify the encoder input tokens (which is corrupted sequences) and record valid length for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens = mask_to_sentinel(tokens, masks, len(tokenizer.vocab))\n",
    "enc_tokens = batcher(masked_tokens)\n",
    "enc_valid_length = np.array([len(tok) for tok in masked_tokens], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get the initial states of decoder by calling `init_states()`. This method includes feeding the corrupted minibatch into the encoder, initializing \"past\" keys and values in every decoder's layer to zero, etc. The returned states is a 4-tuple of encoded results, valid lengths of corrupted sequences, our position (index) in incremental decoding, and past keys and values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = t5mlm.init_states(enc_tokens, enc_valid_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we simply initiate the beam search with a `<pad>` token (token id = 0) for each sample. The beam search will leverage the incremental decoding implemented in `T5Seq2seq` to speed up the inference, though it may still take a while. \n",
    "\n",
    "When the search is done, we can print the results nicely using our helper function, and compare them with the masked tokens (see above). Happily, the pretrianed T5-large gives reasonable (and some time perfect) guess in our toy MLM task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAMPLE 0]\n",
      "(Beam 0)\t<extra_id_0>: in the work\t\n",
      "(Beam 1)\t<extra_id_0>: in work\t\n",
      "(Beam 2)\t<extra_id_0>: in work\t\n",
      "(Beam 3)\t<extra_id_0>: in work\t\n",
      "\n",
      "[SAMPLE 1]\n",
      "(Beam 0)\t<extra_id_0>: state of \t<extra_id_1>: peace is\t<extra_id_2>: people state\t\n",
      "(Beam 1)\t<extra_id_0>: state of \t<extra_id_1>: peace is\t<extra_id_2>: people state\t\n",
      "(Beam 2)\t<extra_id_0>: state of \t<extra_id_1>: peace is\t<extra_id_2>: people state\t\n",
      "(Beam 3)\t<extra_id_0>: state of \t<extra_id_1>: peace is\t<extra_id_2>: people state\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = t5mlm_seacher(np.zeros_like(enc_valid_length), states, enc_valid_length)\n",
    "print_inference(tokenizer, output, noise_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Raffel, C., et al. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\". JMLR 2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
