{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import mxnet as mx\n",
    "from mxnet import np, npx\n",
    "from gluonnlp.attention_cell import gen_self_attn_mask, gen_mem_attn_mask\n",
    "from gluonnlp.data.tokenizers import SentencepieceTokenizer\n",
    "from gluonnlp.models.t5 import T5Model as Gluon_T5\n",
    "from gluonnlp.utils.misc import download, logging_config, sha1sum, naming_convention\n",
    "from transformers import T5Model as HF_T5\n",
    "\n",
    "\n",
    "# these mappings are adapted from huggingface T5 folder\n",
    "T5_PRETRAINED_MODEL_MAP = {\n",
    "    \"t5-small\": \"google_t5_small\",\n",
    "    \"t5-base\": \"google_t5_base\",\n",
    "    \"t5-large\": \"google_t5_large\",\n",
    "    \"t5-3b\": \"google_t5_3B\",\n",
    "    \"t5-11b\": \"google_t5_11B\"\n",
    "}\n",
    "T5_PRETRAINED_CONFIG_MAP = {\n",
    "    \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n",
    "    \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n",
    "    \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n",
    "    \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n",
    "    \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\"\n",
    "}\n",
    "PRETRAINED_VOCAB_MAP = {\n",
    "    \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n",
    "    \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n",
    "    \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n",
    "    \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n",
    "    \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\"\n",
    "}\n",
    "\n",
    "\n",
    "# this mapping only works on \"T5Model\" class from Huggingface and GluonNLP\n",
    "PARAM_MAP = [\n",
    "    # 0.\n",
    "    ('shared.weight', 'input_embedding_layer.weight'), \n",
    "    # 1. encoder / decoder\n",
    "    ('{}.block.0.layer.0.SelfAttention.relative_attention_bias.weight', '{}.relative_position_encoder._rel_pos_embed.weight'), \n",
    "    # 2. encoder / decoder, block/layer #, 0->self_attn_layer_norm / (decoder: 1->cross_attn_layer_norm) / (encoder: 1/decoder: 2)->ffn.layer_norm\n",
    "    ('{}.block.{}.layer.{}.layer_norm.weight', '{}.layers.{}.{}.gamma'), \n",
    "    # 3. encoder / decoder, block/layer #, 0.Self->self / 1.EncDec->cross, q/k/v\n",
    "    ('{}.block.{}.layer.{}Attention.{}.weight', '{}.layers.{}.{}_attn_{}.weight'), \n",
    "    # 4. encoder / decoder, block/layer #, 0.SelfAttention.o->self_attn_proj / (decoder: 1.EncDecAttention.o->cross_attn_proj)\n",
    "    ('{}.block.{}.layer.{}.weight', '{}.layers.{}.{}.weight'), \n",
    "    # 5. encoder / decoder, block/layer #, (encoder: 1 / decoder: 2), wi->ffn_1 / wi_0->gated_ffn_1 / wi_1->ffn_1 / wo->ffn_2\n",
    "    ('{}.block.{}.layer.{}.DenseReluDense.{}.weight', '{}.layers.{}.ffn.{}.weight'), \n",
    "    # 6. encoder / decoder\n",
    "    ('{}.final_layer_norm.weight', '{}.final_layer_norm.gamma'), \n",
    "]\n",
    "\n",
    "\n",
    "def parse_args(): \n",
    "    parser = argparse.ArgumentParser('Convert Huggingface T5 Model to GluonNLP')\n",
    "    parser.add_argument(\n",
    "        'model_name', choices=list(T5_PRETRAINED_MODEL_MAP.keys()), help='Name of pretrained T5 model in Huggingface.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'dest_dir', help='Directory to save converted config, vocab and weights.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test', action='store_true', required=False, default=False, help='Whether to test conversion correctness.'\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def convert_config(args, converted): \n",
    "    print('converting cfg...')\n",
    "    # download config\n",
    "    gluon_cfg = Gluon_T5.get_cfg(T5_PRETRAINED_MODEL_MAP[args.model_name])\n",
    "    with tempfile.TemporaryDirectory() as temp_dir: \n",
    "        hf_cfg_path = os.path.join(temp_dir, 'config.json')\n",
    "        download(\n",
    "            url=T5_PRETRAINED_CONFIG_MAP[args.model_name], \n",
    "            path=hf_cfg_path\n",
    "        )\n",
    "        with open(hf_cfg_path, 'r') as f: \n",
    "            hf_cfg = json.load(f)\n",
    "        os.remove(hf_cfg_path)\n",
    "    # update attributes\n",
    "    cfg = gluon_cfg.clone()\n",
    "    cfg.defrost()\n",
    "    cfg.MODEL.vocab_size = hf_cfg['vocab_size']\n",
    "    cfg.MODEL.d_model = hf_cfg['d_model']\n",
    "    cfg.MODEL.d_kv = hf_cfg['d_kv']\n",
    "    cfg.MODEL.d_ff = hf_cfg['d_ff']\n",
    "    cfg.MODEL.num_layers = hf_cfg['num_layers']\n",
    "    cfg.MODEL.num_heads = hf_cfg['num_heads']\n",
    "    cfg.MODEL.layer_norm_eps = hf_cfg['layer_norm_epsilon']\n",
    "    cfg.MODEL.dropout_prob = hf_cfg['dropout_rate']\n",
    "    cfg.INITIALIZER.init_factor = hf_cfg['initializer_factor']\n",
    "    cfg.freeze()\n",
    "    # save config\n",
    "    config_path = os.path.join(args.dest_dir, '{}.yml'.format(\n",
    "        T5_PRETRAINED_MODEL_MAP[args.model_name]\n",
    "    ))\n",
    "    with open(config_path, 'w') as f: \n",
    "        f.write(cfg.dump())\n",
    "    converted['config'] = config_path\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def convert_vocab(args, converted): \n",
    "    print('converting vocab...')\n",
    "    # at this step we don't add <extra_id>s into the vocab, but just save the original binary file directly\n",
    "    # those pecial tokens are added only when T5 tokenizer gets instantiated from build_t5_tokenizer()\n",
    "    vocab_path = os.path.join(args.dest_dir, '{}.vocab'.format(\n",
    "        T5_PRETRAINED_MODEL_MAP[args.model_name]\n",
    "    ))\n",
    "    download(\n",
    "        url=PRETRAINED_VOCAB_MAP[args.model_name], \n",
    "        path=vocab_path\n",
    "    )\n",
    "    converted['vocab'] = vocab_path\n",
    "\n",
    "\n",
    "def convert_params(args, converted, hf_model, gluon_model): \n",
    "    print('converting parameters...')\n",
    "    # prepare models and parameters\n",
    "    gluon_model.initialize()\n",
    "    hf_params = hf_model.state_dict()\n",
    "    gluon_params = gluon_model.collect_params()\n",
    "    # TODO(yongyi-wu): add sanity check, eg. param #, layer #, ffn activation, etc.\n",
    "    num_layers = gluon_model.num_layers\n",
    "\n",
    "    def convert(hf_param, gluon_param): \n",
    "        gluon_params[gluon_param].set_data(hf_params[hf_param].cpu().numpy())\n",
    "    \n",
    "    # convert parameters\n",
    "    for idx, (hf_key, gluon_key) in enumerate(PARAM_MAP): \n",
    "        if idx == 0: \n",
    "            convert(hf_key, gluon_key)    \n",
    "        elif idx == 1: \n",
    "            for i in ['encoder', 'decoder']: \n",
    "                convert(hf_key.format(i), gluon_key.format(i))\n",
    "        elif idx in [2, 3, 4, 5]: \n",
    "            for stack in ['encoder', 'decoder']: \n",
    "                for layer in range(num_layers): \n",
    "                    if idx == 2: \n",
    "                        if stack == 'encoder': \n",
    "                            L = ['self_attn_layer_norm', 'ffn.layer_norm']\n",
    "                        else: \n",
    "                            L = ['self_attn_layer_norm', 'cross_attn_layer_norm', 'ffn.layer_norm']\n",
    "                        for i, j in enumerate(L): \n",
    "                            convert(hf_key.format(stack, layer, i), gluon_key.format(stack, layer, j))\n",
    "                    elif idx == 3: \n",
    "                        for i in ['q', 'k', 'v']: \n",
    "                            convert(\n",
    "                                hf_key.format(stack, layer, '0.Self', i), \n",
    "                                gluon_key.format(stack, layer, 'self', i)\n",
    "                            )\n",
    "                            if stack == 'decoder': \n",
    "                                convert(\n",
    "                                    hf_key.format(stack, layer, '1.EncDec', i), \n",
    "                                    gluon_key.format(stack, layer, 'cross', i)\n",
    "                                )\n",
    "                    elif idx == 4: \n",
    "                        convert(\n",
    "                            hf_key.format(stack, layer, '0.SelfAttention.o'), \n",
    "                            gluon_key.format(stack, layer, 'self_attn_proj')\n",
    "                        )\n",
    "                        if stack == 'decoder': \n",
    "                            convert(\n",
    "                                hf_key.format(stack, layer, '1.EncDecAttention.o'), \n",
    "                                gluon_key.format(stack, layer, 'cross_attn_proj')\n",
    "                            )\n",
    "                    elif idx == 5:\n",
    "                        if gluon_model.activation == 'relu': \n",
    "                            denses = [('wi', 'ffn_1'), ('wo', 'ffn_2')]\n",
    "                        elif gluon_model.activation == 'gated-gelu': \n",
    "                            denses = [('wi_0', 'gated_ffn_1'), ('wi_1', 'ffn_1'), ('wo', 'ffn_2')]\n",
    "                        else: \n",
    "                            raise ValueError\n",
    "                        i = 1 if stack == 'encoder' else 2\n",
    "                        for j1, j2 in denses: \n",
    "                            convert(\n",
    "                                hf_key.format(stack, layer, i, j1), \n",
    "                                gluon_key.format(stack, layer, j2)\n",
    "                        )\n",
    "        elif idx == 6: \n",
    "            for stack in ['encoder', 'decoder']: \n",
    "                convert(hf_key.format(stack), gluon_key.format(stack))\n",
    "    # save parameters\n",
    "    param_path = os.path.join(args.dest_dir, '{}.params'.format(\n",
    "        T5_PRETRAINED_MODEL_MAP[args.model_name]\n",
    "    ))\n",
    "    gluon_model.save_parameters(param_path)\n",
    "    converted['params'] = param_path\n",
    "    return gluon_model\n",
    "\n",
    "\n",
    "def rename(args, converted): \n",
    "    for item, old_path in converted.items(): \n",
    "        new_name, _ = naming_convention(args.dest_dir, os.path.basename(old_path))\n",
    "        new_path = os.path.join(args.dest_dir, new_name)\n",
    "        shutil.move(old_path, new_path)\n",
    "        logging.info('{} of {} has been converted to {}.'.format(item, args.model_name, new_path))\n",
    "        converted[item] = new_path\n",
    "\n",
    "\n",
    "def test_conversion(args, hf_model, gluon_model): \n",
    "    logging.info('testing conversion...')\n",
    "    # create dummy input\n",
    "    batch_size = 6\n",
    "    src_length = 128\n",
    "    tgt_length = 8\n",
    "    vocab_size = hf_model.shared.weight.shape[0]\n",
    "    src_data = np.random.randint(1, vocab_size, (batch_size, src_length))\n",
    "    src_valid_length = np.random.randint(src_length // 2, src_length, (batch_size,))\n",
    "    tgt_data = np.random.randint(1, vocab_size, (batch_size, tgt_length))\n",
    "    tgt_valid_length = np.random.randint(tgt_length // 2, tgt_length, (batch_size,))\n",
    "    enc_attn_mask = npx.arange_like(src_data, axis=-1) < src_valid_length.reshape(-1, 1)\n",
    "    dec_attn_mask = npx.arange_like(tgt_data, axis=-1) < tgt_valid_length.reshape(-1, 1)\n",
    "    # test T5Model forward pass\n",
    "    hf_model.eval() # disable dropout\n",
    "    hf_out = hf_model(\n",
    "        input_ids=torch.from_numpy(src_data.asnumpy()),  \n",
    "        attention_mask=torch.from_numpy(enc_attn_mask.asnumpy()), \n",
    "        decoder_input_ids=torch.from_numpy(tgt_data.asnumpy()), \n",
    "        decoder_attention_mask=torch.from_numpy(dec_attn_mask.asnumpy())\n",
    "    )['last_hidden_state'].detach().numpy()\n",
    "    gl_out = gluon_model(src_data, src_valid_length, tgt_data, tgt_valid_length)\n",
    "    for i in  range(batch_size):\n",
    "        assert np.allclose(\n",
    "            hf_out[i, :tgt_valid_length[i].item(), :], \n",
    "            gl_out[i, :tgt_valid_length[i].item(), :], \n",
    "            1E-3, \n",
    "            1E-3\n",
    "        )\n",
    "    logging.info('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object): \n",
    "    def __init__(self): \n",
    "        self.model_name = 't5-base'\n",
    "        self.dest_dir = '/home/ubuntu/yongyiw/temp'\n",
    "        self.test = True\n",
    "\n",
    "args = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting vocab...\n",
      "converting cfg...\n",
      "Downloading /tmp/tmpv5c_hkg2/config.json from https://huggingface.co/t5-base/resolve/main/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.20k/1.20k [00:00<00:00, 1.08MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting parameters...\n"
     ]
    }
   ],
   "source": [
    "logging.info('converting T5 model from Huggingface...')\n",
    "if not os.path.exists(args.dest_dir): \n",
    "    os.mkdir(args.dest_dir)\n",
    "converted = {}\n",
    "# convert and save vocab\n",
    "convert_vocab(args, converted)\n",
    "# convert and save config\n",
    "gluon_cfg = convert_config(args, converted)\n",
    "# convert, (test), and save model\n",
    "hf_t5 = HF_T5.from_pretrained(args.model_name)\n",
    "gluon_t5 = Gluon_T5.from_cfg(gluon_cfg)\n",
    "gluon_t5 = convert_params(args, converted, hf_t5, gluon_t5)\n",
    "gluon_t5.hybridize()\n",
    "\n",
    "hf_model, gluon_model = hf_t5, gluon_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# print('testing model...')\n",
    "# create dummy input\n",
    "batch_size = 6\n",
    "src_length = 128\n",
    "tgt_length = 8\n",
    "vocab_size = hf_t5.shared.weight.shape[0]\n",
    "src_data = np.random.randint(1, vocab_size, (batch_size, src_length))\n",
    "src_valid_length = np.random.randint(src_length // 2, src_length, (batch_size,))\n",
    "# src_valid_length = np.full((batch_size,), src_length)\n",
    "tgt_data = np.random.randint(1, vocab_size, (batch_size, tgt_length))\n",
    "tgt_valid_length = np.random.randint(tgt_length // 2, tgt_length, (batch_size,))\n",
    "# tgt_valid_length = np.full((batch_size,), tgt_length)\n",
    "\n",
    "enc_attn_mask = npx.arange_like(src_data, axis=-1) < src_valid_length.reshape(-1, 1)\n",
    "dec_attn_mask = npx.arange_like(tgt_data, axis=-1) < tgt_valid_length.reshape(-1, 1)\n",
    "# enc_attn_mask = gen_self_attn_mask(\n",
    "#     np.expand_dims(src_data, axis=-1), \n",
    "#     src_valid_length, \n",
    "#     attn_type='full'\n",
    "# )\n",
    "# dec_attn_mask = gen_self_attn_mask(\n",
    "#     np.expand_dims(tgt_data, axis=-1), \n",
    "#     tgt_valid_length, \n",
    "#     attn_type='causal'\n",
    "# )\n",
    "\n",
    "# print(src_valid_length)\n",
    "# print(enc_attn_mask)\n",
    "# print()\n",
    "# print(tgt_valid_length)\n",
    "# print(dec_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_out = gluon_model(src_data, src_valid_length, tgt_data, tgt_valid_length)\n",
    "hf_model.eval() # disable dropout and test forward pass\n",
    "hf_out = hf_model(\n",
    "    input_ids=torch.from_numpy(src_data.asnumpy()),  \n",
    "    attention_mask=torch.from_numpy(enc_attn_mask.asnumpy()), \n",
    "    decoder_input_ids=torch.from_numpy(tgt_data.asnumpy()), \n",
    "    decoder_attention_mask=torch.from_numpy(dec_attn_mask.asnumpy())\n",
    ")['last_hidden_state'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:np.allclose is a fallback operator, which is actually using official numpy's implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "for i in  range(batch_size):\n",
    "   assert np.allclose(\n",
    "       hf_out[i, :tgt_valid_length[i].item(), :], \n",
    "       gl_out[i, :tgt_valid_length[i].item(), :], \n",
    "       1E-3, \n",
    "       1E-3\n",
    "   )\n",
    "print('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shared.weight': torch.Size([32128, 768]),\n",
       " 'encoder.embed_tokens.weight': torch.Size([32128, 768]),\n",
       " 'encoder.block.0.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.0.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.0.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.0.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight': torch.Size([32, 12]),\n",
       " 'encoder.block.0.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.0.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.0.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.0.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.1.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.1.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.1.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.1.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.1.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.1.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.1.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.1.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.2.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.2.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.2.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.2.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.2.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.2.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.2.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.2.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.3.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.3.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.3.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.3.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.3.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.3.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.3.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.3.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.4.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.4.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.4.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.4.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.4.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.4.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.4.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.4.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.5.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.5.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.5.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.5.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.5.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.5.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.5.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.5.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.6.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.6.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.6.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.6.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.6.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.6.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.6.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.6.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.7.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.7.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.7.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.7.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.7.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.7.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.7.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.7.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.8.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.8.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.8.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.8.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.8.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.8.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.8.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.8.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.9.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.9.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.9.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.9.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.9.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.9.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.9.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.9.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.10.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.10.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.10.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.10.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.10.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.10.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.10.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.10.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.11.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.11.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.11.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.11.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'encoder.block.11.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.block.11.layer.1.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'encoder.block.11.layer.1.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'encoder.block.11.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'encoder.final_layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.embed_tokens.weight': torch.Size([32128, 768]),\n",
       " 'decoder.block.0.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight': torch.Size([32, 12]),\n",
       " 'decoder.block.0.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.0.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.0.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.0.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.0.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.0.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.1.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.1.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.1.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.1.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.1.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.1.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.2.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.2.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.2.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.2.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.2.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.2.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.3.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.3.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.3.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.3.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.3.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.3.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.4.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.4.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.4.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.4.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.4.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.4.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.5.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.5.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.5.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.5.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.5.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.5.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.6.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.6.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.6.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.6.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.6.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.6.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.7.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.7.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.7.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.7.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.7.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.7.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.8.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.8.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.8.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.8.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.8.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.8.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.9.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.9.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.9.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.9.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.9.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.9.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.10.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.10.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.10.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.10.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.10.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.10.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.11.layer.0.SelfAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.0.SelfAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.0.SelfAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.0.SelfAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.0.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.11.layer.1.EncDecAttention.q.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.1.EncDecAttention.k.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.1.EncDecAttention.v.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.1.EncDecAttention.o.weight': torch.Size([768, 768]),\n",
       " 'decoder.block.11.layer.1.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.block.11.layer.2.DenseReluDense.wi.weight': torch.Size([3072, 768]),\n",
       " 'decoder.block.11.layer.2.DenseReluDense.wo.weight': torch.Size([768, 3072]),\n",
       " 'decoder.block.11.layer.2.layer_norm.weight': torch.Size([768]),\n",
       " 'decoder.final_layer_norm.weight': torch.Size([768])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "for k, v in hf_t5.state_dict().items(): \n",
    "    d[k] = v.shape\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_embedding_layer.weight': Parameter (shape=(32128, 768), dtype=float32),\n",
       " 'encoder.relative_position_encoder._rel_pos_embed.weight': Parameter (shape=(32, 12), dtype=float32),\n",
       " 'encoder.layers.0.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.0.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.0.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.0.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.0.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.0.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.0.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.0.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.0.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.0.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.1.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.1.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.1.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.1.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.1.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.1.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.1.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.1.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.1.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.1.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.2.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.2.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.2.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.2.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.2.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.2.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.2.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.2.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.2.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.2.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.3.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.3.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.3.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.3.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.3.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.3.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.3.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.3.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.3.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.3.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.4.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.4.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.4.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.4.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.4.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.4.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.4.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.4.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.4.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.4.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.5.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.5.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.5.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.5.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.5.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.5.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.5.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.5.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.5.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.5.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.6.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.6.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.6.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.6.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.6.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.6.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.6.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.6.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.6.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.6.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.7.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.7.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.7.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.7.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.7.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.7.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.7.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.7.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.7.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.7.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.8.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.8.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.8.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.8.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.8.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.8.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.8.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.8.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.8.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.8.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.9.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.9.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.9.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.9.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.9.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.9.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.9.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.9.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.9.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.9.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.10.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.10.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.10.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.10.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.10.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.10.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.10.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.10.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.10.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.10.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.11.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.11.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.11.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.11.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.11.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.11.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'encoder.layers.11.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'encoder.layers.11.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'encoder.layers.11.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.layers.11.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.final_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'encoder.final_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.relative_position_encoder._rel_pos_embed.weight': Parameter (shape=(32, 12), dtype=float32),\n",
       " 'decoder.layers.0.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.0.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.0.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.0.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.0.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.0.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.0.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.0.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.0.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.1.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.1.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.1.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.1.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.1.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.1.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.1.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.1.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.1.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.2.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.2.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.2.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.2.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.2.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.2.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.2.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.2.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.2.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.3.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.3.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.3.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.3.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.3.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.3.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.3.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.3.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.3.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.4.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.4.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.4.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.4.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.4.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.4.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.4.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.4.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.4.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.5.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.5.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.5.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.5.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.5.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.5.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.5.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.5.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.5.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.6.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.6.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.6.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.6.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.6.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.6.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.6.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.6.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.6.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.7.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.7.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.7.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.7.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.7.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.7.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.7.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.7.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.7.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.8.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.8.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.8.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.8.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.8.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.8.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.8.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.8.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.8.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.9.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.9.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.9.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.9.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.9.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.9.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.9.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.9.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.9.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.10.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.10.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.10.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.10.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.10.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.10.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.10.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.10.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.10.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.11.self_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.11.self_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.11.self_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.self_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.self_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.self_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.cross_attn_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.11.cross_attn_layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.11.cross_attn_q.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.cross_attn_k.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.cross_attn_v.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.cross_attn_proj.weight': Parameter (shape=(768, 768), dtype=float32),\n",
       " 'decoder.layers.11.ffn.ffn_1.weight': Parameter (shape=(3072, 768), dtype=float32),\n",
       " 'decoder.layers.11.ffn.ffn_2.weight': Parameter (shape=(768, 3072), dtype=float32),\n",
       " 'decoder.layers.11.ffn.layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.layers.11.ffn.layer_norm.beta': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.final_layer_norm.gamma': Parameter (shape=(768,), dtype=float32),\n",
       " 'decoder.final_layer_norm.beta': Parameter (shape=(768,), dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gluon_t5.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
