{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.data.tokenizers import SentencepieceTokenizer\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "gluon = SentencepieceTokenizer(model_path=trans.vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = 100\n",
    "additional_special_tokens = {\n",
    "    'extra_{}_token'.format(i): '<extra_id_{}>'.format(i) for i in range(extra_ids - 1, -1, -1)\n",
    "} # keys must end with \"token\"\n",
    "# https://github.com/dmlc/gluon-nlp/blob/12f6da265237a9ab32feff5e4a446a275f74e5da/src/gluonnlp/data/vocab.py#L205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</s>', '<pad>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/google/sentencepiece/blob/master/doc/special_symbols.md\n",
    "# unable to add extra_token_{1..99} in spm.SentencePieceProcessor constructor\n",
    "\n",
    "# https://github.com/dmlc/gluon-nlp/blob/12f6da265237a9ab32feff5e4a446a275f74e5da/src/gluonnlp/data/tokenizers/sentencepiece.py#L112-L122\n",
    "# unable to add extra_token_{1..99} in SentencepieceTokenizer constructor\n",
    "\n",
    "other_control_tokens_ids = \\\n",
    "            [i for i in range(len(gluon._sp_model))\n",
    "             if gluon._sp_model.is_control(i)]\n",
    "other_control_tokens = set([gluon._sp_model.id_to_piece(ele)\n",
    "                            for ele in other_control_tokens_ids])\n",
    "other_control_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.data.vocab import Vocab\n",
    "\n",
    "gluon._vocab = Vocab(gluon._vocab.all_tokens, **additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <extra_id_0>, Index: 32000\n",
      "Token: <extra_id_1>, Index: 32011\n",
      "Token: <extra_id_2>, Index: 32022\n",
      "Token: <extra_id_3>, Index: 32033\n",
      "Token: <extra_id_4>, Index: 32044\n",
      "Token: <extra_id_5>, Index: 32055\n",
      "Token: <extra_id_6>, Index: 32066\n",
      "Token: <extra_id_7>, Index: 32077\n",
      "Token: <extra_id_8>, Index: 32088\n",
      "Token: <extra_id_9>, Index: 32099\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/dmlc/gluon-nlp/blob/12f6da265237a9ab32feff5e4a446a275f74e5da/src/gluonnlp/data/vocab.py#L204\n",
    "\n",
    "for i in range(10): \n",
    "    token = '<extra_id_{}>'.format(i)\n",
    "    print('Token: {}, Index: {}'.format(token, gluon.vocab(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8774, 1150]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gluon.encode(\"Hello World\", int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8774, 1150, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.encode(\"Hello World\") # </s>: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='t5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceTokenizer(\n",
       "   model_path = /home/ubuntu/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
       "   lowercase = False, nbest = 0, alpha = 0.0\n",
       "   vocab = Vocab(size=32100, unk_token=\"<unk>\", extra_0_token=\"<extra_id_0>\", extra_10_token=\"<extra_id_10>\", extra_11_token=\"<extra_id_11>\", extra_12_token=\"<extra_id_12>\", extra_13_token=\"<extra_id_13>\", extra_14_token=\"<extra_id_14>\", extra_15_token=\"<extra_id_15>\", extra_16_token=\"<extra_id_16>\", extra_17_token=\"<extra_id_17>\", extra_18_token=\"<extra_id_18>\", extra_19_token=\"<extra_id_19>\", extra_1_token=\"<extra_id_1>\", extra_20_token=\"<extra_id_20>\", extra_21_token=\"<extra_id_21>\", extra_22_token=\"<extra_id_22>\", extra_23_token=\"<extra_id_23>\", extra_24_token=\"<extra_id_24>\", extra_25_token=\"<extra_id_25>\", extra_26_token=\"<extra_id_26>\", extra_27_token=\"<extra_id_27>\", extra_28_token=\"<extra_id_28>\", extra_29_token=\"<extra_id_29>\", extra_2_token=\"<extra_id_2>\", extra_30_token=\"<extra_id_30>\", extra_31_token=\"<extra_id_31>\", extra_32_token=\"<extra_id_32>\", extra_33_token=\"<extra_id_33>\", extra_34_token=\"<extra_id_34>\", extra_35_token=\"<extra_id_35>\", extra_36_token=\"<extra_id_36>\", extra_37_token=\"<extra_id_37>\", extra_38_token=\"<extra_id_38>\", extra_39_token=\"<extra_id_39>\", extra_3_token=\"<extra_id_3>\", extra_40_token=\"<extra_id_40>\", extra_41_token=\"<extra_id_41>\", extra_42_token=\"<extra_id_42>\", extra_43_token=\"<extra_id_43>\", extra_44_token=\"<extra_id_44>\", extra_45_token=\"<extra_id_45>\", extra_46_token=\"<extra_id_46>\", extra_47_token=\"<extra_id_47>\", extra_48_token=\"<extra_id_48>\", extra_49_token=\"<extra_id_49>\", extra_4_token=\"<extra_id_4>\", extra_50_token=\"<extra_id_50>\", extra_51_token=\"<extra_id_51>\", extra_52_token=\"<extra_id_52>\", extra_53_token=\"<extra_id_53>\", extra_54_token=\"<extra_id_54>\", extra_55_token=\"<extra_id_55>\", extra_56_token=\"<extra_id_56>\", extra_57_token=\"<extra_id_57>\", extra_58_token=\"<extra_id_58>\", extra_59_token=\"<extra_id_59>\", extra_5_token=\"<extra_id_5>\", extra_60_token=\"<extra_id_60>\", extra_61_token=\"<extra_id_61>\", extra_62_token=\"<extra_id_62>\", extra_63_token=\"<extra_id_63>\", extra_64_token=\"<extra_id_64>\", extra_65_token=\"<extra_id_65>\", extra_66_token=\"<extra_id_66>\", extra_67_token=\"<extra_id_67>\", extra_68_token=\"<extra_id_68>\", extra_69_token=\"<extra_id_69>\", extra_6_token=\"<extra_id_6>\", extra_70_token=\"<extra_id_70>\", extra_71_token=\"<extra_id_71>\", extra_72_token=\"<extra_id_72>\", extra_73_token=\"<extra_id_73>\", extra_74_token=\"<extra_id_74>\", extra_75_token=\"<extra_id_75>\", extra_76_token=\"<extra_id_76>\", extra_77_token=\"<extra_id_77>\", extra_78_token=\"<extra_id_78>\", extra_79_token=\"<extra_id_79>\", extra_7_token=\"<extra_id_7>\", extra_80_token=\"<extra_id_80>\", extra_81_token=\"<extra_id_81>\", extra_82_token=\"<extra_id_82>\", extra_83_token=\"<extra_id_83>\", extra_84_token=\"<extra_id_84>\", extra_85_token=\"<extra_id_85>\", extra_86_token=\"<extra_id_86>\", extra_87_token=\"<extra_id_87>\", extra_88_token=\"<extra_id_88>\", extra_89_token=\"<extra_id_89>\", extra_8_token=\"<extra_id_8>\", extra_90_token=\"<extra_id_90>\", extra_91_token=\"<extra_id_91>\", extra_92_token=\"<extra_id_92>\", extra_93_token=\"<extra_id_93>\", extra_94_token=\"<extra_id_94>\", extra_95_token=\"<extra_id_95>\", extra_96_token=\"<extra_id_96>\", extra_97_token=\"<extra_id_97>\", extra_98_token=\"<extra_id_98>\", extra_99_token=\"<extra_id_99>\", extra_9_token=\"<extra_id_9>\")\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_span_to_unique_sentinel(tokens, noise_mask, vocab):\n",
    "  \"\"\"Replace each run of consecutive noise tokens with a different sentinel.\n",
    "  The idea here is to be able to align the dropped spans in the inputs\n",
    "  with the markers in the targets.\n",
    "  We want to generate training examples like\n",
    "  \"We hold X to be Y that\" -> \"X these truths Y self evident Z\"\n",
    "  Sentinels assigned in decreasing order within the sequence starting at\n",
    "  vocabulary.size - 1.  That is, we appropriate the last tokens in the\n",
    "  vocabulary for additional use as sentinels.\n",
    "  TODO(noam): we may want to try enlarging the vocabulary and leaving room\n",
    "  for the sentinels instead.  However, this requires enlarging the embedding\n",
    "  tables in the model, so that is a bigger change.\n",
    "  Args:\n",
    "    tokens: a 1d integer Tensor\n",
    "    noise_mask: a boolean Tensor with the same shape as tokens\n",
    "    vocabulary: a vocabulary.Vocabulary\n",
    "    seeds: an unused int32 Tensor\n",
    "  Returns:\n",
    "    a Tensor with the same shape and dtype as tokens\n",
    "  \"\"\"\n",
    "  vocab_size = vocab\n",
    "  prev_token_is_noise = tf.pad(noise_mask[:-1], [[1, 0]])\n",
    "\n",
    "  first_noise_tokens = tf.logical_and(\n",
    "      noise_mask, tf.logical_not(prev_token_is_noise))\n",
    "  subsequent_noise_tokens = tf.logical_and(noise_mask, prev_token_is_noise)\n",
    "\n",
    "  sentinel = vocab_size - tf.cumsum(tf.cast(first_noise_tokens, tokens.dtype))\n",
    "\n",
    "  tokens = tf.where(first_noise_tokens, sentinel, tokens)\n",
    "  return tf.boolean_mask(tokens, tf.logical_not(subsequent_noise_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([999,   2,   3,   4, 998,   7, 997], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_span_to_unique_sentinel(\n",
    "    tf.constant([1, 2, 3, 4, 5, 6, 7, 8]), \n",
    "    tf.constant([True, False, False, False, True, True, False, True]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "altr_tk = AlbertTokenizer.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 10975, 126, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "altr_tk(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algl_tk = SentencepieceTokenizer(altr_tk.vocab_file, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10975, 126]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algl_tk.encode(\"Hello World\", int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='albert-base-v2', vocab_size=30000, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<pad>', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "altr_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceTokenizer(\n",
       "   model_path = /home/ubuntu/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n",
       "   lowercase = True, nbest = 0, alpha = 0.0\n",
       "   vocab = Vocab(size=30000, unk_token=\"<unk>\", pad_token=\"<pad>\", other2_token=\"[CLS]\", other0_token=\"[SEP]\", other1_token=\"[MASK]\")\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algl_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
