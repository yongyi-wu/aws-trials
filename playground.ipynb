{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from gluonnlp.data.tokenizers import SentencepieceTokenizer\n",
    "from gluonnlp.data.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_ids = 100\n",
    "# special_tokens = {'extra_{}_token'.format(i): '<extra_id_{}>'.format(i) for i in range(extra_ids)}\n",
    "# special_tokens['eos_token'] = '</s>'\n",
    "# special_tokens['unk_token'] = '<unk>'\n",
    "# special_tokens['pad_token'] = '<pad>'\n",
    "# gluon = SentencepieceTokenizer(\n",
    "#     vocab=Vocab(trans.get_vocab(), **special_tokens)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = 100\n",
    "do_lower = False\n",
    "\n",
    "additional_special_tokens = {'extra{}_token'.format(i): '<extra_id_{}>'.format(i) for i in range(extra_ids - 1, -1, -1)}\n",
    "tokenizer = SentencepieceTokenizer(\n",
    "    model_path=trans.vocab_file,\n",
    "    lowercase=do_lower, \n",
    "    **additional_special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792k/792k [00:00<00:00, 11.7MiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 476k/476k [00:00<00:00, 8.02MiB/s]Downloading /tmp/tmpptcvki94/t5_spm.model from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model...\n",
      "Downloading /tmp/tmpptcvki94/t5_spm_vocab.json from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm_vocab-a9d819.json...\n",
      "\n",
      "['<pad>', '</s>', '<unk>', '<extra_id_99>', '<extra_id_98>', '<extra_id_97>', '<extra_id_96>', '<extra_id_95>', '<extra_id_94>', '<extra_id_93>', '<extra_id_92>', '<extra_id_91>', '<extra_id_90>', '<extra_id_89>', '<extra_id_88>', '<extra_id_87>', '<extra_id_86>', '<extra_id_85>', '<extra_id_84>', '<extra_id_83>', '<extra_id_82>', '<extra_id_81>', '<extra_id_80>', '<extra_id_79>', '<extra_id_78>', '<extra_id_77>', '<extra_id_76>', '<extra_id_75>', '<extra_id_74>', '<extra_id_73>', '<extra_id_72>', '<extra_id_71>', '<extra_id_70>', '<extra_id_69>', '<extra_id_68>', '<extra_id_67>', '<extra_id_66>', '<extra_id_65>', '<extra_id_64>', '<extra_id_63>', '<extra_id_62>', '<extra_id_61>', '<extra_id_60>', '<extra_id_59>', '<extra_id_58>', '<extra_id_57>', '<extra_id_56>', '<extra_id_55>', '<extra_id_54>', '<extra_id_53>', '<extra_id_52>', '<extra_id_51>', '<extra_id_50>', '<extra_id_49>', '<extra_id_48>', '<extra_id_47>', '<extra_id_46>', '<extra_id_45>', '<extra_id_44>', '<extra_id_43>', '<extra_id_42>', '<extra_id_41>', '<extra_id_40>', '<extra_id_39>', '<extra_id_38>', '<extra_id_37>', '<extra_id_36>', '<extra_id_35>', '<extra_id_34>', '<extra_id_33>', '<extra_id_32>', '<extra_id_31>', '<extra_id_30>', '<extra_id_29>', '<extra_id_28>', '<extra_id_27>', '<extra_id_26>', '<extra_id_25>', '<extra_id_24>', '<extra_id_23>', '<extra_id_22>', '<extra_id_21>', '<extra_id_20>', '<extra_id_19>', '<extra_id_18>', '<extra_id_17>', '<extra_id_16>', '<extra_id_15>', '<extra_id_14>', '<extra_id_13>', '<extra_id_12>', '<extra_id_11>', '<extra_id_10>', '<extra_id_9>', '<extra_id_8>', '<extra_id_7>', '<extra_id_6>', '<extra_id_5>', '<extra_id_4>', '<extra_id_3>', '<extra_id_2>', '<extra_id_1>', '<extra_id_0>']\n",
      "32099 32000\n",
      "<pad> 0\n",
      "</s> 1\n",
      "<unk> 2\n",
      "<extra_id_99> 2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2c257bcb3fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "from gluonnlp.base import get_repo_url\n",
    "from gluonnlp.data import load_vocab\n",
    "from gluonnlp.utils.misc import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['‚ñÅHello', ',', '‚ñÅ', 'y', \"'\", 'all', '!', '‚ñÅHow', '‚ñÅare', '‚ñÅyou', '‚ñÅVIII', '‚ñÅ', 'üòÅ', '‚ñÅ', 'üòÅ', '‚ñÅ', 'üòÅ', '‚ñÅ', '?'], ['‚ñÅ', 'Glu', 'on', 'N', 'LP', '‚ñÅis', '‚ñÅgreat', '!', '!!!!!'], ['‚ñÅ', 'Glu', 'on', 'N', 'LP', '-', 'Am', 'a', 'zon', '-', 'H', 'a', 'i', 'bin', '-', 'Le', 'on', 'ard', '-', 'She', 'ng', '-', 'Sh', 'u', 'a', 'i', '-', 'X', 'ing', 'j', 'i', 'an', '.....', '/', ':', '!', '@', '#', '‚ñÅ', \"'\", 'a', 'b', 'c', \"'\"]], [[(0, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 12), (12, 13), (13, 17), (17, 21), (21, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35)], [(0, 0), (0, 3), (3, 5), (5, 6), (6, 8), (8, 11), (11, 17), (17, 18), (18, 23)], [(0, 0), (0, 3), (3, 5), (5, 6), (6, 8), (8, 9), (9, 11), (11, 12), (12, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 22), (22, 23), (23, 25), (25, 27), (27, 30), (30, 31), (31, 34), (34, 36), (36, 37), (37, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 47), (47, 48), (48, 49), (49, 51), (51, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67)]])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as dir_path: \n",
    "    model_path = os.path.join(dir_path, 't5_spm.model')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model',\n",
    "        path=model_path\n",
    "    )\n",
    "    vocab_path = os.path.join(dir_path, 't5_spm_vocab.json')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm_vocab-a9d819.json', \n",
    "        path=vocab_path\n",
    "    )\n",
    "    vocab = (json.load(vocab_path), \n",
    "    # gluon = SentencepieceTokenizer(\n",
    "    #     model_path=model_path, \n",
    "    #     vocab=load_vocab(vocab_path)\n",
    "    # )\n",
    "    # print(gluon._vocab.special_tokens)\n",
    "    # print(gluon._vocab['<extra_id_0>'], gluon._vocab['<extra_id_99>'])\n",
    "    # for token in gluon._vocab.special_tokens:\n",
    "    #     piece_id = gluon._sp_model.piece_to_id(token)\n",
    "    #     print(token, piece_id)\n",
    "    #     if gluon._sp_model.is_unknown(piece_id):\n",
    "    #         assert gluon._vocab[token] == gluon._sp_model.unk_id()\n",
    "    # os.remove(model_path)\n",
    "    # os.remove(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBWORD_TEST_SAMPLES = [\"Hello, y'all! How are you ‚Öß üòÅ üòÅ üòÅ ?\",\n",
    "                        'GluonNLP is greatÔºÅÔºÅÔºÅ!!!',\n",
    "                        \"GluonNLP-Amazon-Haibin-Leonard-Sheng-Shuai-Xingjian...../:!@# 'abc'\"]\n",
    "\n",
    "print(gluon.encode(SUBWORD_TEST_SAMPLES, int))\n",
    "print(trans(SUBWORD_TEST_SAMPLES))\n",
    "\n",
    "print(gluon.encode_with_offsets(SUBWORD_TEST_SAMPLES))\n",
    "\n",
    "ints = gluon.encode(SUBWORD_TEST_SAMPLES, int)\n",
    "gluon.decode(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluon = SentencepieceTokenizer(\n",
    "    model_path=trans.vocab_file, \n",
    ")\n",
    "vocab = gluon._vocab.to_json()\n",
    "vocab = json.loads(vocab)\n",
    "# with open('/home/ubuntu/yongyiw/temp/test_t5spm.json', 'w') as vocab_file: \n",
    "#     json.dump(vocab, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([999,   2,   3,   4, 998,   7, 997], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def noise_span_to_unique_sentinel(tokens, noise_mask, vocab):\n",
    "  \"\"\"Replace each run of consecutive noise tokens with a different sentinel.\n",
    "  The idea here is to be able to align the dropped spans in the inputs\n",
    "  with the markers in the targets.\n",
    "  We want to generate training examples like\n",
    "  \"We hold X to be Y that\" -> \"X these truths Y self evident Z\"\n",
    "  Sentinels assigned in decreasing order within the sequence starting at\n",
    "  vocabulary.size - 1.  That is, we appropriate the last tokens in the\n",
    "  vocabulary for additional use as sentinels.\n",
    "  TODO(noam): we may want to try enlarging the vocabulary and leaving room\n",
    "  for the sentinels instead.  However, this requires enlarging the embedding\n",
    "  tables in the model, so that is a bigger change.\n",
    "  Args:\n",
    "    tokens: a 1d integer Tensor\n",
    "    noise_mask: a boolean Tensor with the same shape as tokens\n",
    "    vocabulary: a vocabulary.Vocabulary\n",
    "    seeds: an unused int32 Tensor\n",
    "  Returns:\n",
    "    a Tensor with the same shape and dtype as tokens\n",
    "  \"\"\"\n",
    "  vocab_size = vocab\n",
    "  prev_token_is_noise = tf.pad(noise_mask[:-1], [[1, 0]])\n",
    "\n",
    "  first_noise_tokens = tf.logical_and(\n",
    "      noise_mask, tf.logical_not(prev_token_is_noise))\n",
    "  subsequent_noise_tokens = tf.logical_and(noise_mask, prev_token_is_noise)\n",
    "\n",
    "  sentinel = vocab_size - tf.cumsum(tf.cast(first_noise_tokens, tokens.dtype))\n",
    "\n",
    "  tokens = tf.where(first_noise_tokens, sentinel, tokens)\n",
    "  return tf.boolean_mask(tokens, tf.logical_not(subsequent_noise_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_span_to_unique_sentinel(\n",
    "    tf.constant([1, 2, 3, 4, 5, 6, 7, 8]), \n",
    "    tf.constant([True, False, False, False, True, True, False, True]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "test() not belongs to an encoder",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-068b19eab4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-220492d5ea5b>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}() not belongs to an encoder'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: test() not belongs to an encoder"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def _assert_decoder(fn): \n",
    "    @functools.wraps(fn)\n",
    "    def wrapper(self, *args, **kwargs): \n",
    "        assert self._is_decoder, '{}() not belongs to an encoder'.format(fn.__name__)\n",
    "        return fn(self, *args, *kwargs)\n",
    "    return wrapper\n",
    "\n",
    "class A(object): \n",
    "    def __init__(self, is_decoder): \n",
    "        self._is_decoder = is_decoder\n",
    "\n",
    "    @_assert_decoder\n",
    "    def test(self): \n",
    "        print(\"Done\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enc = A(False)\n",
    "Enc.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dec = A(True)\n",
    "Dec.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.22122064  0.07740038  0.10434403  0.11839255  0.18917114]\n",
       " [-0.12347414 -0.17710291 -0.04513844  0.05793836 -0.1856082 ]\n",
       " [-0.19768797 -0.02080192  0.02444218 -0.00371607 -0.04877499]\n",
       " [-0.00226173  0.05746142  0.14661263  0.06862904  0.0354961 ]\n",
       " [ 0.10731696  0.01201746 -0.09711102 -0.07756966 -0.07882176]\n",
       " [ 0.07417728 -0.1473444  -0.10730928 -0.10424828 -0.1327885 ]\n",
       " [-0.14749663 -0.0524142   0.12662557  0.08950642 -0.06015945]\n",
       " [ 0.12040559 -0.09712193 -0.05825623  0.03717077  0.09300072]\n",
       " [-0.14225756 -0.05176199  0.20088325  0.02863085  0.05604595]\n",
       " [ 0.09697597 -0.05285374 -0.188909    0.06547912 -0.04548132]]\n",
       "<NDArray 10x5 @cpu(0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(0)\n",
    "\n",
    "layer = mx.gluon.nn.Dense(10, in_units=5, weight_initializer=mx.initializer.Normal(0.1))\n",
    "layer.initialize()\n",
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.22122064  0.07740038  0.10434403  0.11839255  0.18917114]\n",
       " [-0.12347414 -0.17710291 -0.04513844  0.05793836 -0.1856082 ]\n",
       " [-0.19768797 -0.02080192  0.02444218 -0.00371607 -0.04877499]\n",
       " [-0.00226173  0.05746142  0.14661263  0.06862904  0.0354961 ]\n",
       " [ 0.10731696  0.01201746 -0.09711102 -0.07756966 -0.07882176]\n",
       " [ 0.07417728 -0.1473444  -0.10730928 -0.10424828 -0.1327885 ]\n",
       " [-0.14749663 -0.0524142   0.12662557  0.08950642 -0.06015945]\n",
       " [ 0.12040559 -0.09712193 -0.05825623  0.03717077  0.09300072]\n",
       " [-0.14225756 -0.05176199  0.20088325  0.02863085  0.05604595]\n",
       " [ 0.09697597 -0.05285374 -0.188909    0.06547912 -0.04548132]]\n",
       "<NDArray 10x5 @cpu(0)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(0)\n",
    "\n",
    "layer = mx.gluon.nn.Dense(10, in_units=5, weight_initializer=mx.init.Normal(0.1))\n",
    "layer.initialize()\n",
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {\"V1\": 8, \"V2\": 1024, \"V3\": False}\n",
    "B = {\"V4\": 1.0, \"V5\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(V1=0, V2=0, V3=True, V4=0, V5=False): \n",
    "    print(V1, V2, V3, V4, V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "foo() got multiple values for keyword argument 'V5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-7ab1c26d1e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: foo() got multiple values for keyword argument 'V5'"
     ]
    }
   ],
   "source": [
    "foo(V5=False, **A, **B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
