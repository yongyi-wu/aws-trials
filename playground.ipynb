{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "from gluonnlp.data.tokenizers import SentencepieceTokenizer\n",
    "from gluonnlp.data.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /tmp/tmp3ajlomgf/spm.model from https://huggingface.co/t5-base/resolve/main/spiece.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792k/792k [00:00<00:00, 1.82MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from gluonnnlp.\n",
    "from gluonnlp.utils.misc import download\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dir_path: \n",
    "    vocab_path = os.path.join(dir_path, 'spm.model')\n",
    "    download(\n",
    "        url='https://huggingface.co/t5-base/resolve/main/spiece.model', \n",
    "        path=vocab_path\n",
    "    )\n",
    "    spm = SentencepieceTokenizer(vocab_path)\n",
    "    os.remove(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = T5Tokenizer(trans.vocab_file, extra_ids=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceTokenizer(\n",
       "   model_path = /tmp/tmp3ajlomgf/spm.model\n",
       "   lowercase = False, nbest = 0, alpha = 0.0\n",
       "   vocab = Vocab(size=32000, unk_token=\"<unk>\", pad_token=\"<pad>\", eos_token=\"</s>\")\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " '▁',\n",
       " 'X',\n",
       " '.',\n",
       " ',',\n",
       " 's',\n",
       " '▁the',\n",
       " 'a',\n",
       " ':',\n",
       " '▁and',\n",
       " '▁to',\n",
       " '▁of',\n",
       " '▁fill',\n",
       " 'e',\n",
       " '▁in',\n",
       " 't',\n",
       " '-',\n",
       " '▁is',\n",
       " '▁de',\n",
       " '▁for',\n",
       " '’',\n",
       " 'i',\n",
       " '▁that',\n",
       " '▁you',\n",
       " 'd',\n",
       " '▁I',\n",
       " '▁with',\n",
       " 'n',\n",
       " '▁on',\n",
       " \"'\",\n",
       " 'o',\n",
       " '▁are',\n",
       " '▁it',\n",
       " 'en',\n",
       " '▁be',\n",
       " '▁The',\n",
       " '▁as',\n",
       " '▁your',\n",
       " 'l',\n",
       " '▁(',\n",
       " '▁or',\n",
       " '▁have',\n",
       " '▁at',\n",
       " '▁from',\n",
       " '▁an',\n",
       " '▁was',\n",
       " '▁this',\n",
       " 'er',\n",
       " '▁la',\n",
       " 'm',\n",
       " 'r',\n",
       " 'ing',\n",
       " '▁can',\n",
       " '!',\n",
       " '▁will',\n",
       " '▁by',\n",
       " '?',\n",
       " '▁not',\n",
       " 're',\n",
       " ')',\n",
       " '▁we',\n",
       " 'y',\n",
       " '▁und',\n",
       " '▁has',\n",
       " '▁all',\n",
       " '▁die',\n",
       " '▁but',\n",
       " '▁our',\n",
       " '▁their',\n",
       " '▁A',\n",
       " '▁more',\n",
       " '▁un',\n",
       " '▁der',\n",
       " 'c',\n",
       " 'u',\n",
       " 'in',\n",
       " '▁so',\n",
       " '▁they',\n",
       " '▁one',\n",
       " '▁about',\n",
       " '▁my',\n",
       " 'ul',\n",
       " '▁which',\n",
       " 'à',\n",
       " '▁In',\n",
       " '/',\n",
       " 'he',\n",
       " 'f',\n",
       " '▁le',\n",
       " '▁out',\n",
       " '▁also',\n",
       " '▁des',\n",
       " '▁It',\n",
       " '▁up',\n",
       " '▁\"',\n",
       " '▁time',\n",
       " 'ă',\n",
       " 'if',\n",
       " '▁This',\n",
       " '▁We',\n",
       " 'p',\n",
       " '▁do',\n",
       " '–',\n",
       " '▁“',\n",
       " 'on',\n",
       " 'h',\n",
       " '▁si',\n",
       " 'le',\n",
       " '▁les',\n",
       " '▁în',\n",
       " '▁his',\n",
       " '▁who',\n",
       " '▁like',\n",
       " 'b',\n",
       " '▁when',\n",
       " ';',\n",
       " '▁been',\n",
       " '▁other',\n",
       " 'ly',\n",
       " '\"',\n",
       " 'g',\n",
       " '▁cu',\n",
       " '▁care',\n",
       " '▁what',\n",
       " '▁new',\n",
       " 'or',\n",
       " '▁some',\n",
       " '▁get',\n",
       " '▁were',\n",
       " '▁just',\n",
       " '▁there',\n",
       " '▁would',\n",
       " 'S',\n",
       " '▁them',\n",
       " '▁any',\n",
       " ').',\n",
       " 'al',\n",
       " '▁into',\n",
       " '▁me',\n",
       " '▁had',\n",
       " '▁se',\n",
       " '▁make',\n",
       " 'at',\n",
       " '▁than',\n",
       " '▁du',\n",
       " '▁over',\n",
       " '▁You',\n",
       " '▁how',\n",
       " '▁no',\n",
       " '▁people',\n",
       " 'an',\n",
       " '”',\n",
       " 'é',\n",
       " 'it',\n",
       " '▁If',\n",
       " 'k',\n",
       " '▁pe',\n",
       " 'is',\n",
       " '▁her',\n",
       " '▁work',\n",
       " 've',\n",
       " '▁only',\n",
       " '▁may',\n",
       " '▁its',\n",
       " '▁first',\n",
       " '▁most',\n",
       " '▁well',\n",
       " '▁use',\n",
       " '▁zu',\n",
       " '▁pour',\n",
       " 'z',\n",
       " 'il',\n",
       " '▁need',\n",
       " '▁these',\n",
       " '▁din',\n",
       " '▁den',\n",
       " '▁us',\n",
       " 'able',\n",
       " '▁S',\n",
       " '▁mit',\n",
       " '▁very',\n",
       " '▁am',\n",
       " '&',\n",
       " '▁au',\n",
       " '▁many',\n",
       " '▁mai',\n",
       " 'A',\n",
       " 'th',\n",
       " '▁through',\n",
       " '▁pentru',\n",
       " '▁two',\n",
       " '▁von',\n",
       " '▁way',\n",
       " 'll',\n",
       " 'I',\n",
       " '▁ce',\n",
       " '▁și',\n",
       " '▁help',\n",
       " '▁best',\n",
       " '),',\n",
       " 'un',\n",
       " '▁years',\n",
       " '▁2',\n",
       " '▁C',\n",
       " '▁nu',\n",
       " '▁good',\n",
       " 'v',\n",
       " '▁1',\n",
       " 'w',\n",
       " '▁das',\n",
       " '▁ca',\n",
       " '▁where',\n",
       " '▁know',\n",
       " '▁year',\n",
       " '▁He',\n",
       " '▁see',\n",
       " '▁für',\n",
       " '▁auf',\n",
       " '▁3',\n",
       " 'de',\n",
       " 'est',\n",
       " '▁back',\n",
       " '▁such',\n",
       " '▁should',\n",
       " 'x',\n",
       " '▁after',\n",
       " '▁could',\n",
       " '▁ist',\n",
       " '▁now',\n",
       " '▁much',\n",
       " 'and',\n",
       " '...',\n",
       " '▁home',\n",
       " 'to',\n",
       " '▁ein',\n",
       " '▁even',\n",
       " '▁que',\n",
       " '▁day',\n",
       " '▁take',\n",
       " '▁want',\n",
       " '▁For',\n",
       " '▁said',\n",
       " '▁sur',\n",
       " '▁une',\n",
       " '▁să',\n",
       " '▁dans',\n",
       " '▁great',\n",
       " '▁este',\n",
       " '▁because',\n",
       " '▁information',\n",
       " 'ului',\n",
       " '▁find',\n",
       " 'C',\n",
       " '▁she',\n",
       " '▁im',\n",
       " 'ation',\n",
       " '▁then',\n",
       " '▁est',\n",
       " '▁par',\n",
       " '▁used',\n",
       " '▁E',\n",
       " '▁made',\n",
       " '▁So',\n",
       " 'am',\n",
       " '▁eine',\n",
       " '▁şi',\n",
       " '▁business',\n",
       " '▁right',\n",
       " '▁here',\n",
       " '▁being',\n",
       " '▁B',\n",
       " '▁those',\n",
       " '▁before',\n",
       " '▁And',\n",
       " '▁P',\n",
       " 'ers',\n",
       " '▁don',\n",
       " 'B',\n",
       " '▁life',\n",
       " '▁go',\n",
       " '▁As',\n",
       " '▁M',\n",
       " '▁each',\n",
       " '▁qui',\n",
       " '▁place',\n",
       " 'com',\n",
       " 'ant',\n",
       " '▁sich',\n",
       " '▁There',\n",
       " 'ar',\n",
       " '▁Sie',\n",
       " '▁own',\n",
       " '▁part',\n",
       " 'ent',\n",
       " '▁world',\n",
       " 'ment',\n",
       " '▁while',\n",
       " '▁But',\n",
       " '▁around',\n",
       " '▁L',\n",
       " 'us',\n",
       " '▁plus',\n",
       " '▁To',\n",
       " '▁5',\n",
       " '▁high',\n",
       " '▁long',\n",
       " 'D',\n",
       " '▁D',\n",
       " '▁really',\n",
       " '▁nicht',\n",
       " '▁Le',\n",
       " '▁service',\n",
       " '▁4',\n",
       " '▁different',\n",
       " '▁Die',\n",
       " '▁think',\n",
       " '—',\n",
       " '▁auch',\n",
       " '▁look',\n",
       " '▁both',\n",
       " 'lor',\n",
       " '▁down',\n",
       " 'ten',\n",
       " '▁La',\n",
       " '▁off',\n",
       " '▁vous',\n",
       " '▁They',\n",
       " 'M',\n",
       " '▁pas',\n",
       " '▁data',\n",
       " '▁T',\n",
       " '▁love',\n",
       " '▁every',\n",
       " '▁10',\n",
       " '▁last',\n",
       " '▁same',\n",
       " '▁using',\n",
       " '▁free',\n",
       " '▁dem',\n",
       " '▁still',\n",
       " 'ate',\n",
       " 'ist',\n",
       " '▁between',\n",
       " 'P',\n",
       " 'be',\n",
       " '▁available',\n",
       " 'man',\n",
       " '▁company',\n",
       " '▁G',\n",
       " '▁experience',\n",
       " '▁going',\n",
       " '▁site',\n",
       " 'j',\n",
       " 'are',\n",
       " '▁set',\n",
       " '2',\n",
       " '▁system',\n",
       " '▁important',\n",
       " '▁few',\n",
       " '▁fi',\n",
       " 'ich',\n",
       " '▁What',\n",
       " '▁services',\n",
       " '▁under',\n",
       " '▁When',\n",
       " '▁online',\n",
       " '▁New',\n",
       " '▁come',\n",
       " '▁provide',\n",
       " 'F',\n",
       " '▁team',\n",
       " '▁always',\n",
       " '▁De',\n",
       " '▁că',\n",
       " '▁him',\n",
       " '▁F',\n",
       " '▁things',\n",
       " '▁including',\n",
       " '▁support',\n",
       " '▁number',\n",
       " 'T',\n",
       " '▁during',\n",
       " '▁family',\n",
       " '▁little',\n",
       " '▁three',\n",
       " '▁water',\n",
       " '▁man',\n",
       " '▁An',\n",
       " 'based',\n",
       " '▁R',\n",
       " '▁sau',\n",
       " '▁avec',\n",
       " '▁better',\n",
       " '▁„',\n",
       " '▁too',\n",
       " 'ge',\n",
       " '▁must',\n",
       " '▁per',\n",
       " 'ele',\n",
       " '▁oder',\n",
       " 'au',\n",
       " '▁aus',\n",
       " '▁werden',\n",
       " '▁does',\n",
       " '▁without',\n",
       " '▁ou',\n",
       " '▁design',\n",
       " '▁va',\n",
       " '▁did',\n",
       " '▁O',\n",
       " '▁U',\n",
       " 'up',\n",
       " '▁end',\n",
       " '▁local',\n",
       " '▁next',\n",
       " '▁sure',\n",
       " '▁lot',\n",
       " '▁Re',\n",
       " '▁top',\n",
       " '▁Our',\n",
       " '▁small',\n",
       " '▁full',\n",
       " '▁something',\n",
       " 'ung',\n",
       " '▁vor',\n",
       " 'E',\n",
       " '▁give',\n",
       " '▁might',\n",
       " '▁another',\n",
       " '▁6',\n",
       " '▁All',\n",
       " '▁process',\n",
       " 'L',\n",
       " '▁found',\n",
       " '▁sind',\n",
       " '▁since',\n",
       " '▁With',\n",
       " 'K',\n",
       " 'um',\n",
       " '▁within',\n",
       " '▁post',\n",
       " '▁car',\n",
       " 'une',\n",
       " '▁N',\n",
       " '▁J',\n",
       " 'ic',\n",
       " 'R',\n",
       " 'ter',\n",
       " 'ur',\n",
       " '▁She',\n",
       " '▁public',\n",
       " '▁keep',\n",
       " '▁H',\n",
       " '▁order',\n",
       " '▁start',\n",
       " 'ez',\n",
       " '▁‘',\n",
       " 'uri',\n",
       " '▁20',\n",
       " '▁On',\n",
       " '▁offer',\n",
       " '▁quality',\n",
       " '▁working',\n",
       " '▁No',\n",
       " '▁That',\n",
       " '▁game',\n",
       " '▁bei',\n",
       " '▁today',\n",
       " '▁never',\n",
       " '▁week',\n",
       " '▁St',\n",
       " '▁feel',\n",
       " '▁put',\n",
       " '▁website',\n",
       " 'Y',\n",
       " '▁days',\n",
       " '▁program',\n",
       " '▁looking',\n",
       " '▁K',\n",
       " '▁students',\n",
       " '▁create',\n",
       " '▁change',\n",
       " '▁book',\n",
       " 'ity',\n",
       " '▁At',\n",
       " '▁possible',\n",
       " '▁sunt',\n",
       " '▁7',\n",
       " '▁real',\n",
       " '▁al',\n",
       " '▁making',\n",
       " '▁Be',\n",
       " '▁products',\n",
       " '▁case',\n",
       " '▁school',\n",
       " '▁say',\n",
       " 'area',\n",
       " '▁My',\n",
       " '▁point',\n",
       " '▁als',\n",
       " '▁children',\n",
       " '▁course',\n",
       " '▁show',\n",
       " '▁8',\n",
       " '▁These',\n",
       " '▁18',\n",
       " '▁large',\n",
       " 'co',\n",
       " '▁über',\n",
       " '▁second',\n",
       " '▁market',\n",
       " '▁fost',\n",
       " '▁easy',\n",
       " '▁plan',\n",
       " '▁project',\n",
       " 'G',\n",
       " 'W',\n",
       " '3',\n",
       " '▁son',\n",
       " 'la',\n",
       " '▁face',\n",
       " '▁needs',\n",
       " 'ch',\n",
       " '▁personal',\n",
       " 'me',\n",
       " '▁sont',\n",
       " '▁je',\n",
       " '▁non',\n",
       " '▁got',\n",
       " '▁Do',\n",
       " 'the',\n",
       " '▁health',\n",
       " '▁special',\n",
       " '.\"',\n",
       " '1',\n",
       " 'den',\n",
       " '▁state',\n",
       " '▁open',\n",
       " '▁money',\n",
       " '▁again',\n",
       " '▁food',\n",
       " '▁page',\n",
       " '▁together',\n",
       " 'age',\n",
       " '▁qu',\n",
       " 'hat',\n",
       " '▁ver',\n",
       " '▁W',\n",
       " '▁away',\n",
       " '▁wird',\n",
       " '▁until',\n",
       " 'V',\n",
       " '▁pre',\n",
       " '▁One',\n",
       " '▁product',\n",
       " '▁often',\n",
       " '▁wir',\n",
       " '▁nach',\n",
       " '▁include',\n",
       " '▁um',\n",
       " '▁room',\n",
       " '▁group',\n",
       " '▁name',\n",
       " 'ce',\n",
       " 'H',\n",
       " 'N',\n",
       " '▁person',\n",
       " '▁social',\n",
       " '▁list',\n",
       " '▁How',\n",
       " '▁why',\n",
       " '▁community',\n",
       " '▁contact',\n",
       " '\\xad',\n",
       " '▁co',\n",
       " '▁play',\n",
       " '▁having',\n",
       " '▁power',\n",
       " '▁call',\n",
       " '▁against',\n",
       " '▁become',\n",
       " '▁cost',\n",
       " '▁V',\n",
       " '▁research',\n",
       " '▁12',\n",
       " '▁wie',\n",
       " 'der',\n",
       " '▁thing',\n",
       " '▁along',\n",
       " '4',\n",
       " '▁access',\n",
       " '▁level',\n",
       " '▁price',\n",
       " '▁einen',\n",
       " '▁side',\n",
       " '▁Un',\n",
       " '▁means',\n",
       " '(',\n",
       " '▁big',\n",
       " '▁God',\n",
       " '▁dass',\n",
       " 'im',\n",
       " '▁30',\n",
       " '▁event',\n",
       " '▁development',\n",
       " '▁form',\n",
       " '▁read',\n",
       " '▁hand',\n",
       " '▁control',\n",
       " '▁However',\n",
       " '▁done',\n",
       " '▁job',\n",
       " '▁hard',\n",
       " '▁war',\n",
       " '▁area',\n",
       " '▁add',\n",
       " '▁votre',\n",
       " '▁live',\n",
       " '▁range',\n",
       " '▁After',\n",
       " '▁Les',\n",
       " '▁far',\n",
       " 'ver',\n",
       " '▁old',\n",
       " '▁perfect',\n",
       " '▁15',\n",
       " '▁space',\n",
       " '▁house',\n",
       " 'ine',\n",
       " '▁enough',\n",
       " '0',\n",
       " '▁several',\n",
       " 'The',\n",
       " 'mm',\n",
       " '▁University',\n",
       " '▁diese',\n",
       " '▁Co',\n",
       " '▁comes',\n",
       " '▁across',\n",
       " '▁already',\n",
       " ',”',\n",
       " '▁body',\n",
       " '▁Das',\n",
       " '▁einer',\n",
       " '▁left',\n",
       " '▁future',\n",
       " '▁times',\n",
       " '▁dar',\n",
       " '▁simple',\n",
       " 'ry',\n",
       " '▁getting',\n",
       " '▁try',\n",
       " 'ți',\n",
       " 'ness',\n",
       " '▁makes',\n",
       " '▁past',\n",
       " 'ca',\n",
       " '▁light',\n",
       " '▁Der',\n",
       " '▁run',\n",
       " '▁four',\n",
       " 'ance',\n",
       " '▁ever',\n",
       " '▁einem',\n",
       " '▁below',\n",
       " 'O',\n",
       " '▁9',\n",
       " '▁learn',\n",
       " 'out',\n",
       " '▁video',\n",
       " '▁etc',\n",
       " '▁«',\n",
       " '▁zum',\n",
       " '▁kann',\n",
       " '▁minutes',\n",
       " '▁example',\n",
       " '▁nous',\n",
       " '▁Se',\n",
       " '▁sie',\n",
       " '▁industry',\n",
       " '▁problem',\n",
       " 'J',\n",
       " '▁country',\n",
       " '▁fact',\n",
       " '▁type',\n",
       " 'ner',\n",
       " '▁companies',\n",
       " '▁line',\n",
       " '▁city',\n",
       " '▁check',\n",
       " '▁doing',\n",
       " 'elle',\n",
       " '▁fun',\n",
       " '▁En',\n",
       " '▁Your',\n",
       " 'ling',\n",
       " '▁share',\n",
       " 'ile',\n",
       " '▁actually',\n",
       " '▁value',\n",
       " 'zi',\n",
       " '▁ab',\n",
       " '▁offers',\n",
       " '▁less',\n",
       " '▁night',\n",
       " '▁Dr',\n",
       " '▁started',\n",
       " '▁least',\n",
       " '▁short',\n",
       " '▁main',\n",
       " '▁single',\n",
       " '▁though',\n",
       " '▁prin',\n",
       " 'time',\n",
       " '▁hours',\n",
       " '▁others',\n",
       " '▁called',\n",
       " '▁visit',\n",
       " '▁bit',\n",
       " 'ée',\n",
       " '▁customers',\n",
       " '▁music',\n",
       " '▁members',\n",
       " 'ies',\n",
       " '▁pay',\n",
       " 'nd',\n",
       " '▁once',\n",
       " 'gen',\n",
       " '▁können',\n",
       " '▁low',\n",
       " '▁durch',\n",
       " '▁story',\n",
       " '▁understand',\n",
       " '“',\n",
       " '▁Am',\n",
       " '▁didn',\n",
       " '▁content',\n",
       " 'son',\n",
       " '▁building',\n",
       " '▁result',\n",
       " '▁aux',\n",
       " '▁complete',\n",
       " '▁doesn',\n",
       " '▁haben',\n",
       " '▁questions',\n",
       " 'line',\n",
       " '▁technology',\n",
       " '▁Pro',\n",
       " '▁current',\n",
       " '▁won',\n",
       " '▁let',\n",
       " '▁features',\n",
       " '▁please',\n",
       " '5',\n",
       " '▁above',\n",
       " 'ive',\n",
       " '▁management',\n",
       " '▁lui',\n",
       " 'her',\n",
       " '▁training',\n",
       " '▁everything',\n",
       " '▁noch',\n",
       " '▁came',\n",
       " '▁web',\n",
       " '▁ensure',\n",
       " '▁months',\n",
       " '▁art',\n",
       " '▁sub',\n",
       " '▁million',\n",
       " '▁professional',\n",
       " '▁results',\n",
       " '▁kind',\n",
       " '▁season',\n",
       " '▁unique',\n",
       " 'ze',\n",
       " '▁enjoy',\n",
       " '▁early',\n",
       " '▁major',\n",
       " '▁yet',\n",
       " '▁Ver',\n",
       " 'one',\n",
       " '▁media',\n",
       " '▁[',\n",
       " '▁property',\n",
       " '▁beautiful',\n",
       " '▁given',\n",
       " '▁due',\n",
       " '▁government',\n",
       " '▁nur',\n",
       " '▁email',\n",
       " '▁total',\n",
       " '▁natural',\n",
       " '▁test',\n",
       " '▁provides',\n",
       " '▁various',\n",
       " '▁American',\n",
       " '▁moment',\n",
       " '▁air',\n",
       " '▁idea',\n",
       " '▁known',\n",
       " '▁Il',\n",
       " '▁friends',\n",
       " '▁final',\n",
       " '▁buy',\n",
       " '▁specific',\n",
       " '▁issues',\n",
       " '▁took',\n",
       " '▁mind',\n",
       " '▁study',\n",
       " '▁addition',\n",
       " '▁size',\n",
       " '▁pro',\n",
       " '▁film',\n",
       " '▁pot',\n",
       " '▁thought',\n",
       " '▁tell',\n",
       " '▁While',\n",
       " '▁head',\n",
       " '▁clients',\n",
       " '▁performance',\n",
       " '▁question',\n",
       " '▁whether',\n",
       " '▁certain',\n",
       " '▁model',\n",
       " '▁following',\n",
       " '▁energy',\n",
       " '▁office',\n",
       " '▁whole',\n",
       " '▁bring',\n",
       " '▁required',\n",
       " 'ţi',\n",
       " '▁date',\n",
       " '_',\n",
       " 'que',\n",
       " '▁da',\n",
       " '▁US',\n",
       " '▁taking',\n",
       " 'go',\n",
       " '▁living',\n",
       " '▁someone',\n",
       " '▁heart',\n",
       " '▁key',\n",
       " '▁areas',\n",
       " '▁says',\n",
       " '▁2018',\n",
       " '▁month',\n",
       " '▁Er',\n",
       " 'ste',\n",
       " '▁11',\n",
       " '▁front',\n",
       " '▁Now',\n",
       " '▁class',\n",
       " '▁choose',\n",
       " 'pe',\n",
       " '▁further',\n",
       " '▁believe',\n",
       " 'of',\n",
       " '▁among',\n",
       " 'sch',\n",
       " '▁child',\n",
       " '▁aber',\n",
       " '▁Please',\n",
       " 'rea',\n",
       " '▁later',\n",
       " '▁amount',\n",
       " 'ice',\n",
       " '▁National',\n",
       " '▁style',\n",
       " '▁tout',\n",
       " '▁staff',\n",
       " '▁white',\n",
       " '▁ge',\n",
       " '▁five',\n",
       " '▁blog',\n",
       " '▁designed',\n",
       " '▁went',\n",
       " '▁Da',\n",
       " '▁general',\n",
       " '▁rest',\n",
       " '▁zur',\n",
       " '▁quite',\n",
       " 'per',\n",
       " '▁customer',\n",
       " '▁close',\n",
       " '▁Some',\n",
       " '▁women',\n",
       " '▁move',\n",
       " '▁software',\n",
       " '▁Ein',\n",
       " '▁Ab',\n",
       " '▁history',\n",
       " '▁either',\n",
       " '▁seen',\n",
       " '▁card',\n",
       " '▁City',\n",
       " '▁hope',\n",
       " '▁16',\n",
       " 'és',\n",
       " 'va',\n",
       " '▁Al',\n",
       " '▁especially',\n",
       " '▁view',\n",
       " 'men',\n",
       " '▁account',\n",
       " '▁needed',\n",
       " '▁United',\n",
       " ']',\n",
       " '▁yourself',\n",
       " '▁100',\n",
       " '▁receive',\n",
       " '▁ideas',\n",
       " '▁writing',\n",
       " '▁simply',\n",
       " '▁present',\n",
       " '▁continue',\n",
       " '▁application',\n",
       " '▁build',\n",
       " '▁turn',\n",
       " 'ated',\n",
       " '▁everyone',\n",
       " 'cette',\n",
       " '▁bien',\n",
       " 'less',\n",
       " '▁Si',\n",
       " '▁original',\n",
       " '8',\n",
       " '▁individual',\n",
       " 'tre',\n",
       " '▁works',\n",
       " '▁options',\n",
       " '▁May',\n",
       " '▁Not',\n",
       " '▁report',\n",
       " 'mer',\n",
       " '▁human',\n",
       " '▁provided',\n",
       " '▁By',\n",
       " '▁series',\n",
       " '7',\n",
       " '▁modern',\n",
       " '▁meet',\n",
       " '▁50',\n",
       " '▁25',\n",
       " '▁color',\n",
       " '▁download',\n",
       " '▁Here',\n",
       " '6',\n",
       " '▁poate',\n",
       " '▁În',\n",
       " '▁phone',\n",
       " '▁likely',\n",
       " '▁table',\n",
       " '▁ma',\n",
       " '▁Or',\n",
       " 'Z',\n",
       " '▁19',\n",
       " '▁insurance',\n",
       " '▁anything',\n",
       " '▁search',\n",
       " '▁Ge',\n",
       " '▁issue',\n",
       " '▁includes',\n",
       " '▁clear',\n",
       " 'les',\n",
       " '▁almost',\n",
       " 'ilor',\n",
       " '▁14',\n",
       " 'by',\n",
       " '▁Du',\n",
       " '▁mais',\n",
       " 'ier',\n",
       " '▁law',\n",
       " '▁added',\n",
       " '▁con',\n",
       " ',\"',\n",
       " '▁ago',\n",
       " '▁His',\n",
       " '▁points',\n",
       " '▁mult',\n",
       " '▁financial',\n",
       " '▁problems',\n",
       " '▁however',\n",
       " '▁events',\n",
       " '▁half',\n",
       " 'ard',\n",
       " '▁ask',\n",
       " '▁version',\n",
       " 'end',\n",
       " '▁created',\n",
       " '▁lead',\n",
       " '▁focus',\n",
       " '▁increase',\n",
       " 'ex',\n",
       " '▁allow',\n",
       " '▁extra',\n",
       " '▁24',\n",
       " '▁credit',\n",
       " '▁production',\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.vocab.all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# extra_ids = 100\n",
    "# id_len = math.ceil(math.log(extra_ids, 10))\n",
    "special_tokens = {'extra_{}_token'.format(str(i).zfill(id_len)): '<extra_id_{}>'.format(str(i).zfill(id_len)) for i in range(extra_ids)}\n",
    "special_tokens['eos_token'] = '</s>'\n",
    "special_tokens['unk_token'] = '<unk>'\n",
    "special_tokens['pad_token'] = '<pad>'\n",
    "# # gluon = SentencepieceTokenizer(\n",
    "# #     vocab=Vocab(trans.get_vocab(), **special_tokens)\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans._extra_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '</s>': 1,\n",
       " '<unk>': 2,\n",
       " '▁': 3,\n",
       " 'X': 4,\n",
       " '.': 5,\n",
       " ',': 6,\n",
       " 's': 7,\n",
       " '▁the': 8,\n",
       " 'a': 9,\n",
       " ':': 10,\n",
       " '▁and': 11,\n",
       " '▁to': 12,\n",
       " '▁of': 13,\n",
       " '▁fill': 14,\n",
       " 'e': 15,\n",
       " '▁in': 16,\n",
       " 't': 17,\n",
       " '-': 18,\n",
       " '▁is': 19,\n",
       " '▁de': 20,\n",
       " '▁for': 21,\n",
       " '’': 22,\n",
       " 'i': 23,\n",
       " '▁that': 24,\n",
       " '▁you': 25,\n",
       " 'd': 26,\n",
       " '▁I': 27,\n",
       " '▁with': 28,\n",
       " 'n': 29,\n",
       " '▁on': 30,\n",
       " \"'\": 31,\n",
       " 'o': 32,\n",
       " '▁are': 33,\n",
       " '▁it': 34,\n",
       " 'en': 35,\n",
       " '▁be': 36,\n",
       " '▁The': 37,\n",
       " '▁as': 38,\n",
       " '▁your': 39,\n",
       " 'l': 40,\n",
       " '▁(': 41,\n",
       " '▁or': 42,\n",
       " '▁have': 43,\n",
       " '▁at': 44,\n",
       " '▁from': 45,\n",
       " '▁an': 46,\n",
       " '▁was': 47,\n",
       " '▁this': 48,\n",
       " 'er': 49,\n",
       " '▁la': 50,\n",
       " 'm': 51,\n",
       " 'r': 52,\n",
       " 'ing': 53,\n",
       " '▁can': 54,\n",
       " '!': 55,\n",
       " '▁will': 56,\n",
       " '▁by': 57,\n",
       " '?': 58,\n",
       " '▁not': 59,\n",
       " 're': 60,\n",
       " ')': 61,\n",
       " '▁we': 62,\n",
       " 'y': 63,\n",
       " '▁und': 64,\n",
       " '▁has': 65,\n",
       " '▁all': 66,\n",
       " '▁die': 67,\n",
       " '▁but': 68,\n",
       " '▁our': 69,\n",
       " '▁their': 70,\n",
       " '▁A': 71,\n",
       " '▁more': 72,\n",
       " '▁un': 73,\n",
       " '▁der': 74,\n",
       " 'c': 75,\n",
       " 'u': 76,\n",
       " 'in': 77,\n",
       " '▁so': 78,\n",
       " '▁they': 79,\n",
       " '▁one': 80,\n",
       " '▁about': 81,\n",
       " '▁my': 82,\n",
       " 'ul': 83,\n",
       " '▁which': 84,\n",
       " 'à': 85,\n",
       " '▁In': 86,\n",
       " '/': 87,\n",
       " 'he': 88,\n",
       " 'f': 89,\n",
       " '▁le': 90,\n",
       " '▁out': 91,\n",
       " '▁also': 92,\n",
       " '▁des': 93,\n",
       " '▁It': 94,\n",
       " '▁up': 95,\n",
       " '▁\"': 96,\n",
       " '▁time': 97,\n",
       " 'ă': 98,\n",
       " 'if': 99,\n",
       " '▁This': 100,\n",
       " '▁We': 101,\n",
       " 'p': 102,\n",
       " '▁do': 103,\n",
       " '–': 104,\n",
       " '▁“': 105,\n",
       " 'on': 106,\n",
       " 'h': 107,\n",
       " '▁si': 108,\n",
       " 'le': 109,\n",
       " '▁les': 110,\n",
       " '▁în': 111,\n",
       " '▁his': 112,\n",
       " '▁who': 113,\n",
       " '▁like': 114,\n",
       " 'b': 115,\n",
       " '▁when': 116,\n",
       " ';': 117,\n",
       " '▁been': 118,\n",
       " '▁other': 119,\n",
       " 'ly': 120,\n",
       " '\"': 121,\n",
       " 'g': 122,\n",
       " '▁cu': 123,\n",
       " '▁care': 124,\n",
       " '▁what': 125,\n",
       " '▁new': 126,\n",
       " 'or': 127,\n",
       " '▁some': 128,\n",
       " '▁get': 129,\n",
       " '▁were': 130,\n",
       " '▁just': 131,\n",
       " '▁there': 132,\n",
       " '▁would': 133,\n",
       " 'S': 134,\n",
       " '▁them': 135,\n",
       " '▁any': 136,\n",
       " ').': 137,\n",
       " 'al': 138,\n",
       " '▁into': 139,\n",
       " '▁me': 140,\n",
       " '▁had': 141,\n",
       " '▁se': 142,\n",
       " '▁make': 143,\n",
       " 'at': 144,\n",
       " '▁than': 145,\n",
       " '▁du': 146,\n",
       " '▁over': 147,\n",
       " '▁You': 148,\n",
       " '▁how': 149,\n",
       " '▁no': 150,\n",
       " '▁people': 151,\n",
       " 'an': 152,\n",
       " '”': 153,\n",
       " 'é': 154,\n",
       " 'it': 155,\n",
       " '▁If': 156,\n",
       " 'k': 157,\n",
       " '▁pe': 158,\n",
       " 'is': 159,\n",
       " '▁her': 160,\n",
       " '▁work': 161,\n",
       " 've': 162,\n",
       " '▁only': 163,\n",
       " '▁may': 164,\n",
       " '▁its': 165,\n",
       " '▁first': 166,\n",
       " '▁most': 167,\n",
       " '▁well': 168,\n",
       " '▁use': 169,\n",
       " '▁zu': 170,\n",
       " '▁pour': 171,\n",
       " 'z': 172,\n",
       " 'il': 173,\n",
       " '▁need': 174,\n",
       " '▁these': 175,\n",
       " '▁din': 176,\n",
       " '▁den': 177,\n",
       " '▁us': 178,\n",
       " 'able': 179,\n",
       " '▁S': 180,\n",
       " '▁mit': 181,\n",
       " '▁very': 182,\n",
       " '▁am': 183,\n",
       " '&': 184,\n",
       " '▁au': 185,\n",
       " '▁many': 186,\n",
       " '▁mai': 187,\n",
       " 'A': 188,\n",
       " 'th': 189,\n",
       " '▁through': 190,\n",
       " '▁pentru': 191,\n",
       " '▁two': 192,\n",
       " '▁von': 193,\n",
       " '▁way': 194,\n",
       " 'll': 195,\n",
       " 'I': 196,\n",
       " '▁ce': 197,\n",
       " '▁și': 198,\n",
       " '▁help': 199,\n",
       " '▁best': 200,\n",
       " '),': 201,\n",
       " 'un': 202,\n",
       " '▁years': 203,\n",
       " '▁2': 204,\n",
       " '▁C': 205,\n",
       " '▁nu': 206,\n",
       " '▁good': 207,\n",
       " 'v': 208,\n",
       " '▁1': 209,\n",
       " 'w': 210,\n",
       " '▁das': 211,\n",
       " '▁ca': 212,\n",
       " '▁where': 213,\n",
       " '▁know': 214,\n",
       " '▁year': 215,\n",
       " '▁He': 216,\n",
       " '▁see': 217,\n",
       " '▁für': 218,\n",
       " '▁auf': 219,\n",
       " '▁3': 220,\n",
       " 'de': 221,\n",
       " 'est': 222,\n",
       " '▁back': 223,\n",
       " '▁such': 224,\n",
       " '▁should': 225,\n",
       " 'x': 226,\n",
       " '▁after': 227,\n",
       " '▁could': 228,\n",
       " '▁ist': 229,\n",
       " '▁now': 230,\n",
       " '▁much': 231,\n",
       " 'and': 232,\n",
       " '...': 233,\n",
       " '▁home': 234,\n",
       " 'to': 235,\n",
       " '▁ein': 236,\n",
       " '▁even': 237,\n",
       " '▁que': 238,\n",
       " '▁day': 239,\n",
       " '▁take': 240,\n",
       " '▁want': 241,\n",
       " '▁For': 242,\n",
       " '▁said': 243,\n",
       " '▁sur': 244,\n",
       " '▁une': 245,\n",
       " '▁să': 246,\n",
       " '▁dans': 247,\n",
       " '▁great': 248,\n",
       " '▁este': 249,\n",
       " '▁because': 250,\n",
       " '▁information': 251,\n",
       " 'ului': 252,\n",
       " '▁find': 253,\n",
       " 'C': 254,\n",
       " '▁she': 255,\n",
       " '▁im': 256,\n",
       " 'ation': 257,\n",
       " '▁then': 258,\n",
       " '▁est': 259,\n",
       " '▁par': 260,\n",
       " '▁used': 261,\n",
       " '▁E': 262,\n",
       " '▁made': 263,\n",
       " '▁So': 264,\n",
       " 'am': 265,\n",
       " '▁eine': 266,\n",
       " '▁şi': 267,\n",
       " '▁business': 268,\n",
       " '▁right': 269,\n",
       " '▁here': 270,\n",
       " '▁being': 271,\n",
       " '▁B': 272,\n",
       " '▁those': 273,\n",
       " '▁before': 274,\n",
       " '▁And': 275,\n",
       " '▁P': 276,\n",
       " 'ers': 277,\n",
       " '▁don': 278,\n",
       " 'B': 279,\n",
       " '▁life': 280,\n",
       " '▁go': 281,\n",
       " '▁As': 282,\n",
       " '▁M': 283,\n",
       " '▁each': 284,\n",
       " '▁qui': 285,\n",
       " '▁place': 286,\n",
       " 'com': 287,\n",
       " 'ant': 288,\n",
       " '▁sich': 289,\n",
       " '▁There': 290,\n",
       " 'ar': 291,\n",
       " '▁Sie': 292,\n",
       " '▁own': 293,\n",
       " '▁part': 294,\n",
       " 'ent': 295,\n",
       " '▁world': 296,\n",
       " 'ment': 297,\n",
       " '▁while': 298,\n",
       " '▁But': 299,\n",
       " '▁around': 300,\n",
       " '▁L': 301,\n",
       " 'us': 302,\n",
       " '▁plus': 303,\n",
       " '▁To': 304,\n",
       " '▁5': 305,\n",
       " '▁high': 306,\n",
       " '▁long': 307,\n",
       " 'D': 308,\n",
       " '▁D': 309,\n",
       " '▁really': 310,\n",
       " '▁nicht': 311,\n",
       " '▁Le': 312,\n",
       " '▁service': 313,\n",
       " '▁4': 314,\n",
       " '▁different': 315,\n",
       " '▁Die': 316,\n",
       " '▁think': 317,\n",
       " '—': 318,\n",
       " '▁auch': 319,\n",
       " '▁look': 320,\n",
       " '▁both': 321,\n",
       " 'lor': 322,\n",
       " '▁down': 323,\n",
       " 'ten': 324,\n",
       " '▁La': 325,\n",
       " '▁off': 326,\n",
       " '▁vous': 327,\n",
       " '▁They': 328,\n",
       " 'M': 329,\n",
       " '▁pas': 330,\n",
       " '▁data': 331,\n",
       " '▁T': 332,\n",
       " '▁love': 333,\n",
       " '▁every': 334,\n",
       " '▁10': 335,\n",
       " '▁last': 336,\n",
       " '▁same': 337,\n",
       " '▁using': 338,\n",
       " '▁free': 339,\n",
       " '▁dem': 340,\n",
       " '▁still': 341,\n",
       " 'ate': 342,\n",
       " 'ist': 343,\n",
       " '▁between': 344,\n",
       " 'P': 345,\n",
       " 'be': 346,\n",
       " '▁available': 347,\n",
       " 'man': 348,\n",
       " '▁company': 349,\n",
       " '▁G': 350,\n",
       " '▁experience': 351,\n",
       " '▁going': 352,\n",
       " '▁site': 353,\n",
       " 'j': 354,\n",
       " 'are': 355,\n",
       " '▁set': 356,\n",
       " '2': 357,\n",
       " '▁system': 358,\n",
       " '▁important': 359,\n",
       " '▁few': 360,\n",
       " '▁fi': 361,\n",
       " 'ich': 362,\n",
       " '▁What': 363,\n",
       " '▁services': 364,\n",
       " '▁under': 365,\n",
       " '▁When': 366,\n",
       " '▁online': 367,\n",
       " '▁New': 368,\n",
       " '▁come': 369,\n",
       " '▁provide': 370,\n",
       " 'F': 371,\n",
       " '▁team': 372,\n",
       " '▁always': 373,\n",
       " '▁De': 374,\n",
       " '▁că': 375,\n",
       " '▁him': 376,\n",
       " '▁F': 377,\n",
       " '▁things': 378,\n",
       " '▁including': 379,\n",
       " '▁support': 380,\n",
       " '▁number': 381,\n",
       " 'T': 382,\n",
       " '▁during': 383,\n",
       " '▁family': 384,\n",
       " '▁little': 385,\n",
       " '▁three': 386,\n",
       " '▁water': 387,\n",
       " '▁man': 388,\n",
       " '▁An': 389,\n",
       " 'based': 390,\n",
       " '▁R': 391,\n",
       " '▁sau': 392,\n",
       " '▁avec': 393,\n",
       " '▁better': 394,\n",
       " '▁„': 395,\n",
       " '▁too': 396,\n",
       " 'ge': 397,\n",
       " '▁must': 398,\n",
       " '▁per': 399,\n",
       " 'ele': 400,\n",
       " '▁oder': 401,\n",
       " 'au': 402,\n",
       " '▁aus': 403,\n",
       " '▁werden': 404,\n",
       " '▁does': 405,\n",
       " '▁without': 406,\n",
       " '▁ou': 407,\n",
       " '▁design': 408,\n",
       " '▁va': 409,\n",
       " '▁did': 410,\n",
       " '▁O': 411,\n",
       " '▁U': 412,\n",
       " 'up': 413,\n",
       " '▁end': 414,\n",
       " '▁local': 415,\n",
       " '▁next': 416,\n",
       " '▁sure': 417,\n",
       " '▁lot': 418,\n",
       " '▁Re': 419,\n",
       " '▁top': 420,\n",
       " '▁Our': 421,\n",
       " '▁small': 422,\n",
       " '▁full': 423,\n",
       " '▁something': 424,\n",
       " 'ung': 425,\n",
       " '▁vor': 426,\n",
       " 'E': 427,\n",
       " '▁give': 428,\n",
       " '▁might': 429,\n",
       " '▁another': 430,\n",
       " '▁6': 431,\n",
       " '▁All': 432,\n",
       " '▁process': 433,\n",
       " 'L': 434,\n",
       " '▁found': 435,\n",
       " '▁sind': 436,\n",
       " '▁since': 437,\n",
       " '▁With': 438,\n",
       " 'K': 439,\n",
       " 'um': 440,\n",
       " '▁within': 441,\n",
       " '▁post': 442,\n",
       " '▁car': 443,\n",
       " 'une': 444,\n",
       " '▁N': 445,\n",
       " '▁J': 446,\n",
       " 'ic': 447,\n",
       " 'R': 448,\n",
       " 'ter': 449,\n",
       " 'ur': 450,\n",
       " '▁She': 451,\n",
       " '▁public': 452,\n",
       " '▁keep': 453,\n",
       " '▁H': 454,\n",
       " '▁order': 455,\n",
       " '▁start': 456,\n",
       " 'ez': 457,\n",
       " '▁‘': 458,\n",
       " 'uri': 459,\n",
       " '▁20': 460,\n",
       " '▁On': 461,\n",
       " '▁offer': 462,\n",
       " '▁quality': 463,\n",
       " '▁working': 464,\n",
       " '▁No': 465,\n",
       " '▁That': 466,\n",
       " '▁game': 467,\n",
       " '▁bei': 468,\n",
       " '▁today': 469,\n",
       " '▁never': 470,\n",
       " '▁week': 471,\n",
       " '▁St': 472,\n",
       " '▁feel': 473,\n",
       " '▁put': 474,\n",
       " '▁website': 475,\n",
       " 'Y': 476,\n",
       " '▁days': 477,\n",
       " '▁program': 478,\n",
       " '▁looking': 479,\n",
       " '▁K': 480,\n",
       " '▁students': 481,\n",
       " '▁create': 482,\n",
       " '▁change': 483,\n",
       " '▁book': 484,\n",
       " 'ity': 485,\n",
       " '▁At': 486,\n",
       " '▁possible': 487,\n",
       " '▁sunt': 488,\n",
       " '▁7': 489,\n",
       " '▁real': 490,\n",
       " '▁al': 491,\n",
       " '▁making': 492,\n",
       " '▁Be': 493,\n",
       " '▁products': 494,\n",
       " '▁case': 495,\n",
       " '▁school': 496,\n",
       " '▁say': 497,\n",
       " 'area': 498,\n",
       " '▁My': 499,\n",
       " '▁point': 500,\n",
       " '▁als': 501,\n",
       " '▁children': 502,\n",
       " '▁course': 503,\n",
       " '▁show': 504,\n",
       " '▁8': 505,\n",
       " '▁These': 506,\n",
       " '▁18': 507,\n",
       " '▁large': 508,\n",
       " 'co': 509,\n",
       " '▁über': 510,\n",
       " '▁second': 511,\n",
       " '▁market': 512,\n",
       " '▁fost': 513,\n",
       " '▁easy': 514,\n",
       " '▁plan': 515,\n",
       " '▁project': 516,\n",
       " 'G': 517,\n",
       " 'W': 518,\n",
       " '3': 519,\n",
       " '▁son': 520,\n",
       " 'la': 521,\n",
       " '▁face': 522,\n",
       " '▁needs': 523,\n",
       " 'ch': 524,\n",
       " '▁personal': 525,\n",
       " 'me': 526,\n",
       " '▁sont': 527,\n",
       " '▁je': 528,\n",
       " '▁non': 529,\n",
       " '▁got': 530,\n",
       " '▁Do': 531,\n",
       " 'the': 532,\n",
       " '▁health': 533,\n",
       " '▁special': 534,\n",
       " '.\"': 535,\n",
       " '1': 536,\n",
       " 'den': 537,\n",
       " '▁state': 538,\n",
       " '▁open': 539,\n",
       " '▁money': 540,\n",
       " '▁again': 541,\n",
       " '▁food': 542,\n",
       " '▁page': 543,\n",
       " '▁together': 544,\n",
       " 'age': 545,\n",
       " '▁qu': 546,\n",
       " 'hat': 547,\n",
       " '▁ver': 548,\n",
       " '▁W': 549,\n",
       " '▁away': 550,\n",
       " '▁wird': 551,\n",
       " '▁until': 552,\n",
       " 'V': 553,\n",
       " '▁pre': 554,\n",
       " '▁One': 555,\n",
       " '▁product': 556,\n",
       " '▁often': 557,\n",
       " '▁wir': 558,\n",
       " '▁nach': 559,\n",
       " '▁include': 560,\n",
       " '▁um': 561,\n",
       " '▁room': 562,\n",
       " '▁group': 563,\n",
       " '▁name': 564,\n",
       " 'ce': 565,\n",
       " 'H': 566,\n",
       " 'N': 567,\n",
       " '▁person': 568,\n",
       " '▁social': 569,\n",
       " '▁list': 570,\n",
       " '▁How': 571,\n",
       " '▁why': 572,\n",
       " '▁community': 573,\n",
       " '▁contact': 574,\n",
       " '\\xad': 575,\n",
       " '▁co': 576,\n",
       " '▁play': 577,\n",
       " '▁having': 578,\n",
       " '▁power': 579,\n",
       " '▁call': 580,\n",
       " '▁against': 581,\n",
       " '▁become': 582,\n",
       " '▁cost': 583,\n",
       " '▁V': 584,\n",
       " '▁research': 585,\n",
       " '▁12': 586,\n",
       " '▁wie': 587,\n",
       " 'der': 588,\n",
       " '▁thing': 589,\n",
       " '▁along': 590,\n",
       " '4': 591,\n",
       " '▁access': 592,\n",
       " '▁level': 593,\n",
       " '▁price': 594,\n",
       " '▁einen': 595,\n",
       " '▁side': 596,\n",
       " '▁Un': 597,\n",
       " '▁means': 598,\n",
       " '(': 599,\n",
       " '▁big': 600,\n",
       " '▁God': 601,\n",
       " '▁dass': 602,\n",
       " 'im': 603,\n",
       " '▁30': 604,\n",
       " '▁event': 605,\n",
       " '▁development': 606,\n",
       " '▁form': 607,\n",
       " '▁read': 608,\n",
       " '▁hand': 609,\n",
       " '▁control': 610,\n",
       " '▁However': 611,\n",
       " '▁done': 612,\n",
       " '▁job': 613,\n",
       " '▁hard': 614,\n",
       " '▁war': 615,\n",
       " '▁area': 616,\n",
       " '▁add': 617,\n",
       " '▁votre': 618,\n",
       " '▁live': 619,\n",
       " '▁range': 620,\n",
       " '▁After': 621,\n",
       " '▁Les': 622,\n",
       " '▁far': 623,\n",
       " 'ver': 624,\n",
       " '▁old': 625,\n",
       " '▁perfect': 626,\n",
       " '▁15': 627,\n",
       " '▁space': 628,\n",
       " '▁house': 629,\n",
       " 'ine': 630,\n",
       " '▁enough': 631,\n",
       " '0': 632,\n",
       " '▁several': 633,\n",
       " 'The': 634,\n",
       " 'mm': 635,\n",
       " '▁University': 636,\n",
       " '▁diese': 637,\n",
       " '▁Co': 638,\n",
       " '▁comes': 639,\n",
       " '▁across': 640,\n",
       " '▁already': 641,\n",
       " ',”': 642,\n",
       " '▁body': 643,\n",
       " '▁Das': 644,\n",
       " '▁einer': 645,\n",
       " '▁left': 646,\n",
       " '▁future': 647,\n",
       " '▁times': 648,\n",
       " '▁dar': 649,\n",
       " '▁simple': 650,\n",
       " 'ry': 651,\n",
       " '▁getting': 652,\n",
       " '▁try': 653,\n",
       " 'ți': 654,\n",
       " 'ness': 655,\n",
       " '▁makes': 656,\n",
       " '▁past': 657,\n",
       " 'ca': 658,\n",
       " '▁light': 659,\n",
       " '▁Der': 660,\n",
       " '▁run': 661,\n",
       " '▁four': 662,\n",
       " 'ance': 663,\n",
       " '▁ever': 664,\n",
       " '▁einem': 665,\n",
       " '▁below': 666,\n",
       " 'O': 667,\n",
       " '▁9': 668,\n",
       " '▁learn': 669,\n",
       " 'out': 670,\n",
       " '▁video': 671,\n",
       " '▁etc': 672,\n",
       " '▁«': 673,\n",
       " '▁zum': 674,\n",
       " '▁kann': 675,\n",
       " '▁minutes': 676,\n",
       " '▁example': 677,\n",
       " '▁nous': 678,\n",
       " '▁Se': 679,\n",
       " '▁sie': 680,\n",
       " '▁industry': 681,\n",
       " '▁problem': 682,\n",
       " 'J': 683,\n",
       " '▁country': 684,\n",
       " '▁fact': 685,\n",
       " '▁type': 686,\n",
       " 'ner': 687,\n",
       " '▁companies': 688,\n",
       " '▁line': 689,\n",
       " '▁city': 690,\n",
       " '▁check': 691,\n",
       " '▁doing': 692,\n",
       " 'elle': 693,\n",
       " '▁fun': 694,\n",
       " '▁En': 695,\n",
       " '▁Your': 696,\n",
       " 'ling': 697,\n",
       " '▁share': 698,\n",
       " 'ile': 699,\n",
       " '▁actually': 700,\n",
       " '▁value': 701,\n",
       " 'zi': 702,\n",
       " '▁ab': 703,\n",
       " '▁offers': 704,\n",
       " '▁less': 705,\n",
       " '▁night': 706,\n",
       " '▁Dr': 707,\n",
       " '▁started': 708,\n",
       " '▁least': 709,\n",
       " '▁short': 710,\n",
       " '▁main': 711,\n",
       " '▁single': 712,\n",
       " '▁though': 713,\n",
       " '▁prin': 714,\n",
       " 'time': 715,\n",
       " '▁hours': 716,\n",
       " '▁others': 717,\n",
       " '▁called': 718,\n",
       " '▁visit': 719,\n",
       " '▁bit': 720,\n",
       " 'ée': 721,\n",
       " '▁customers': 722,\n",
       " '▁music': 723,\n",
       " '▁members': 724,\n",
       " 'ies': 725,\n",
       " '▁pay': 726,\n",
       " 'nd': 727,\n",
       " '▁once': 728,\n",
       " 'gen': 729,\n",
       " '▁können': 730,\n",
       " '▁low': 731,\n",
       " '▁durch': 732,\n",
       " '▁story': 733,\n",
       " '▁understand': 734,\n",
       " '“': 735,\n",
       " '▁Am': 736,\n",
       " '▁didn': 737,\n",
       " '▁content': 738,\n",
       " 'son': 739,\n",
       " '▁building': 740,\n",
       " '▁result': 741,\n",
       " '▁aux': 742,\n",
       " '▁complete': 743,\n",
       " '▁doesn': 744,\n",
       " '▁haben': 745,\n",
       " '▁questions': 746,\n",
       " 'line': 747,\n",
       " '▁technology': 748,\n",
       " '▁Pro': 749,\n",
       " '▁current': 750,\n",
       " '▁won': 751,\n",
       " '▁let': 752,\n",
       " '▁features': 753,\n",
       " '▁please': 754,\n",
       " '5': 755,\n",
       " '▁above': 756,\n",
       " 'ive': 757,\n",
       " '▁management': 758,\n",
       " '▁lui': 759,\n",
       " 'her': 760,\n",
       " '▁training': 761,\n",
       " '▁everything': 762,\n",
       " '▁noch': 763,\n",
       " '▁came': 764,\n",
       " '▁web': 765,\n",
       " '▁ensure': 766,\n",
       " '▁months': 767,\n",
       " '▁art': 768,\n",
       " '▁sub': 769,\n",
       " '▁million': 770,\n",
       " '▁professional': 771,\n",
       " '▁results': 772,\n",
       " '▁kind': 773,\n",
       " '▁season': 774,\n",
       " '▁unique': 775,\n",
       " 'ze': 776,\n",
       " '▁enjoy': 777,\n",
       " '▁early': 778,\n",
       " '▁major': 779,\n",
       " '▁yet': 780,\n",
       " '▁Ver': 781,\n",
       " 'one': 782,\n",
       " '▁media': 783,\n",
       " '▁[': 784,\n",
       " '▁property': 785,\n",
       " '▁beautiful': 786,\n",
       " '▁given': 787,\n",
       " '▁due': 788,\n",
       " '▁government': 789,\n",
       " '▁nur': 790,\n",
       " '▁email': 791,\n",
       " '▁total': 792,\n",
       " '▁natural': 793,\n",
       " '▁test': 794,\n",
       " '▁provides': 795,\n",
       " '▁various': 796,\n",
       " '▁American': 797,\n",
       " '▁moment': 798,\n",
       " '▁air': 799,\n",
       " '▁idea': 800,\n",
       " '▁known': 801,\n",
       " '▁Il': 802,\n",
       " '▁friends': 803,\n",
       " '▁final': 804,\n",
       " '▁buy': 805,\n",
       " '▁specific': 806,\n",
       " '▁issues': 807,\n",
       " '▁took': 808,\n",
       " '▁mind': 809,\n",
       " '▁study': 810,\n",
       " '▁addition': 811,\n",
       " '▁size': 812,\n",
       " '▁pro': 813,\n",
       " '▁film': 814,\n",
       " '▁pot': 815,\n",
       " '▁thought': 816,\n",
       " '▁tell': 817,\n",
       " '▁While': 818,\n",
       " '▁head': 819,\n",
       " '▁clients': 820,\n",
       " '▁performance': 821,\n",
       " '▁question': 822,\n",
       " '▁whether': 823,\n",
       " '▁certain': 824,\n",
       " '▁model': 825,\n",
       " '▁following': 826,\n",
       " '▁energy': 827,\n",
       " '▁office': 828,\n",
       " '▁whole': 829,\n",
       " '▁bring': 830,\n",
       " '▁required': 831,\n",
       " 'ţi': 832,\n",
       " '▁date': 833,\n",
       " '_': 834,\n",
       " 'que': 835,\n",
       " '▁da': 836,\n",
       " '▁US': 837,\n",
       " '▁taking': 838,\n",
       " 'go': 839,\n",
       " '▁living': 840,\n",
       " '▁someone': 841,\n",
       " '▁heart': 842,\n",
       " '▁key': 843,\n",
       " '▁areas': 844,\n",
       " '▁says': 845,\n",
       " '▁2018': 846,\n",
       " '▁month': 847,\n",
       " '▁Er': 848,\n",
       " 'ste': 849,\n",
       " '▁11': 850,\n",
       " '▁front': 851,\n",
       " '▁Now': 852,\n",
       " '▁class': 853,\n",
       " '▁choose': 854,\n",
       " 'pe': 855,\n",
       " '▁further': 856,\n",
       " '▁believe': 857,\n",
       " 'of': 858,\n",
       " '▁among': 859,\n",
       " 'sch': 860,\n",
       " '▁child': 861,\n",
       " '▁aber': 862,\n",
       " '▁Please': 863,\n",
       " 'rea': 864,\n",
       " '▁later': 865,\n",
       " '▁amount': 866,\n",
       " 'ice': 867,\n",
       " '▁National': 868,\n",
       " '▁style': 869,\n",
       " '▁tout': 870,\n",
       " '▁staff': 871,\n",
       " '▁white': 872,\n",
       " '▁ge': 873,\n",
       " '▁five': 874,\n",
       " '▁blog': 875,\n",
       " '▁designed': 876,\n",
       " '▁went': 877,\n",
       " '▁Da': 878,\n",
       " '▁general': 879,\n",
       " '▁rest': 880,\n",
       " '▁zur': 881,\n",
       " '▁quite': 882,\n",
       " 'per': 883,\n",
       " '▁customer': 884,\n",
       " '▁close': 885,\n",
       " '▁Some': 886,\n",
       " '▁women': 887,\n",
       " '▁move': 888,\n",
       " '▁software': 889,\n",
       " '▁Ein': 890,\n",
       " '▁Ab': 891,\n",
       " '▁history': 892,\n",
       " '▁either': 893,\n",
       " '▁seen': 894,\n",
       " '▁card': 895,\n",
       " '▁City': 896,\n",
       " '▁hope': 897,\n",
       " '▁16': 898,\n",
       " 'és': 899,\n",
       " 'va': 900,\n",
       " '▁Al': 901,\n",
       " '▁especially': 902,\n",
       " '▁view': 903,\n",
       " 'men': 904,\n",
       " '▁account': 905,\n",
       " '▁needed': 906,\n",
       " '▁United': 907,\n",
       " ']': 908,\n",
       " '▁yourself': 909,\n",
       " '▁100': 910,\n",
       " '▁receive': 911,\n",
       " '▁ideas': 912,\n",
       " '▁writing': 913,\n",
       " '▁simply': 914,\n",
       " '▁present': 915,\n",
       " '▁continue': 916,\n",
       " '▁application': 917,\n",
       " '▁build': 918,\n",
       " '▁turn': 919,\n",
       " 'ated': 920,\n",
       " '▁everyone': 921,\n",
       " 'cette': 922,\n",
       " '▁bien': 923,\n",
       " 'less': 924,\n",
       " '▁Si': 925,\n",
       " '▁original': 926,\n",
       " '8': 927,\n",
       " '▁individual': 928,\n",
       " 'tre': 929,\n",
       " '▁works': 930,\n",
       " '▁options': 931,\n",
       " '▁May': 932,\n",
       " '▁Not': 933,\n",
       " '▁report': 934,\n",
       " 'mer': 935,\n",
       " '▁human': 936,\n",
       " '▁provided': 937,\n",
       " '▁By': 938,\n",
       " '▁series': 939,\n",
       " '7': 940,\n",
       " '▁modern': 941,\n",
       " '▁meet': 942,\n",
       " '▁50': 943,\n",
       " '▁25': 944,\n",
       " '▁color': 945,\n",
       " '▁download': 946,\n",
       " '▁Here': 947,\n",
       " '6': 948,\n",
       " '▁poate': 949,\n",
       " '▁În': 950,\n",
       " '▁phone': 951,\n",
       " '▁likely': 952,\n",
       " '▁table': 953,\n",
       " '▁ma': 954,\n",
       " '▁Or': 955,\n",
       " 'Z': 956,\n",
       " '▁19': 957,\n",
       " '▁insurance': 958,\n",
       " '▁anything': 959,\n",
       " '▁search': 960,\n",
       " '▁Ge': 961,\n",
       " '▁issue': 962,\n",
       " '▁includes': 963,\n",
       " '▁clear': 964,\n",
       " 'les': 965,\n",
       " '▁almost': 966,\n",
       " 'ilor': 967,\n",
       " '▁14': 968,\n",
       " 'by': 969,\n",
       " '▁Du': 970,\n",
       " '▁mais': 971,\n",
       " 'ier': 972,\n",
       " '▁law': 973,\n",
       " '▁added': 974,\n",
       " '▁con': 975,\n",
       " ',\"': 976,\n",
       " '▁ago': 977,\n",
       " '▁His': 978,\n",
       " '▁points': 979,\n",
       " '▁mult': 980,\n",
       " '▁financial': 981,\n",
       " '▁problems': 982,\n",
       " '▁however': 983,\n",
       " '▁events': 984,\n",
       " '▁half': 985,\n",
       " 'ard': 986,\n",
       " '▁ask': 987,\n",
       " '▁version': 988,\n",
       " 'end': 989,\n",
       " '▁created': 990,\n",
       " '▁lead': 991,\n",
       " '▁focus': 992,\n",
       " '▁increase': 993,\n",
       " 'ex': 994,\n",
       " '▁allow': 995,\n",
       " '▁extra': 996,\n",
       " '▁24': 997,\n",
       " '▁credit': 998,\n",
       " '▁production': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(trans.get_vocab().keys())\n",
    "\n",
    "# trans.pad_token\n",
    "# trans.eos_token\n",
    "# trans.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = trans._extra_ids\n",
    "do_lower = False\n",
    "\n",
    "special_tokens = {'extra{}_token'.format(i): '<extra_id_{}>'.format(i) for i in range(extra_ids - 1, -1, -1)}\n",
    "special_tokens['eos_token'] = trans.eos_token\n",
    "special_tokens['unk_token'] = trans.unk_token\n",
    "special_tokens['pad_token'] = trans.pad_token\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(\n",
    "    model_path=trans.vocab_file, \n",
    "    vocab=Vocab(list(trans.get_vocab().keys()), **special_tokens), \n",
    "    lowercase=do_lower\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceTokenizer(\n",
       "   model_path = /home/ubuntu/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
       "   lowercase = False, nbest = 0, alpha = 0.0\n",
       "   vocab = Vocab(size=32100, unk_token=\"<unk>\", pad_token=\"<pad>\", eos_token=\"</s>\", extra99_token=\"<extra_id_99>\", extra98_token=\"<extra_id_98>\", extra97_token=\"<extra_id_97>\", extra96_token=\"<extra_id_96>\", extra95_token=\"<extra_id_95>\", extra94_token=\"<extra_id_94>\", extra93_token=\"<extra_id_93>\", extra92_token=\"<extra_id_92>\", extra91_token=\"<extra_id_91>\", extra90_token=\"<extra_id_90>\", extra89_token=\"<extra_id_89>\", extra88_token=\"<extra_id_88>\", extra87_token=\"<extra_id_87>\", extra86_token=\"<extra_id_86>\", extra85_token=\"<extra_id_85>\", extra84_token=\"<extra_id_84>\", extra83_token=\"<extra_id_83>\", extra82_token=\"<extra_id_82>\", extra81_token=\"<extra_id_81>\", extra80_token=\"<extra_id_80>\", extra79_token=\"<extra_id_79>\", extra78_token=\"<extra_id_78>\", extra77_token=\"<extra_id_77>\", extra76_token=\"<extra_id_76>\", extra75_token=\"<extra_id_75>\", extra74_token=\"<extra_id_74>\", extra73_token=\"<extra_id_73>\", extra72_token=\"<extra_id_72>\", extra71_token=\"<extra_id_71>\", extra70_token=\"<extra_id_70>\", extra69_token=\"<extra_id_69>\", extra68_token=\"<extra_id_68>\", extra67_token=\"<extra_id_67>\", extra66_token=\"<extra_id_66>\", extra65_token=\"<extra_id_65>\", extra64_token=\"<extra_id_64>\", extra63_token=\"<extra_id_63>\", extra62_token=\"<extra_id_62>\", extra61_token=\"<extra_id_61>\", extra60_token=\"<extra_id_60>\", extra59_token=\"<extra_id_59>\", extra58_token=\"<extra_id_58>\", extra57_token=\"<extra_id_57>\", extra56_token=\"<extra_id_56>\", extra55_token=\"<extra_id_55>\", extra54_token=\"<extra_id_54>\", extra53_token=\"<extra_id_53>\", extra52_token=\"<extra_id_52>\", extra51_token=\"<extra_id_51>\", extra50_token=\"<extra_id_50>\", extra49_token=\"<extra_id_49>\", extra48_token=\"<extra_id_48>\", extra47_token=\"<extra_id_47>\", extra46_token=\"<extra_id_46>\", extra45_token=\"<extra_id_45>\", extra44_token=\"<extra_id_44>\", extra43_token=\"<extra_id_43>\", extra42_token=\"<extra_id_42>\", extra41_token=\"<extra_id_41>\", extra40_token=\"<extra_id_40>\", extra39_token=\"<extra_id_39>\", extra38_token=\"<extra_id_38>\", extra37_token=\"<extra_id_37>\", extra36_token=\"<extra_id_36>\", extra35_token=\"<extra_id_35>\", extra34_token=\"<extra_id_34>\", extra33_token=\"<extra_id_33>\", extra32_token=\"<extra_id_32>\", extra31_token=\"<extra_id_31>\", extra30_token=\"<extra_id_30>\", extra29_token=\"<extra_id_29>\", extra28_token=\"<extra_id_28>\", extra27_token=\"<extra_id_27>\", extra26_token=\"<extra_id_26>\", extra25_token=\"<extra_id_25>\", extra24_token=\"<extra_id_24>\", extra23_token=\"<extra_id_23>\", extra22_token=\"<extra_id_22>\", extra21_token=\"<extra_id_21>\", extra20_token=\"<extra_id_20>\", extra19_token=\"<extra_id_19>\", extra18_token=\"<extra_id_18>\", extra17_token=\"<extra_id_17>\", extra16_token=\"<extra_id_16>\", extra15_token=\"<extra_id_15>\", extra14_token=\"<extra_id_14>\", extra13_token=\"<extra_id_13>\", extra12_token=\"<extra_id_12>\", extra11_token=\"<extra_id_11>\", extra10_token=\"<extra_id_10>\", extra9_token=\"<extra_id_9>\", extra8_token=\"<extra_id_8>\", extra7_token=\"<extra_id_7>\", extra6_token=\"<extra_id_6>\", extra5_token=\"<extra_id_5>\", extra4_token=\"<extra_id_4>\", extra3_token=\"<extra_id_3>\", extra2_token=\"<extra_id_2>\", extra1_token=\"<extra_id_1>\", extra0_token=\"<extra_id_0>\")\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792k/792k [00:00<00:00, 11.7MiB/s]\n",
      "100%|██████████| 476k/476k [00:00<00:00, 8.02MiB/s]Downloading /tmp/tmpptcvki94/t5_spm.model from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model...\n",
      "Downloading /tmp/tmpptcvki94/t5_spm_vocab.json from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm_vocab-a9d819.json...\n",
      "\n",
      "['<pad>', '</s>', '<unk>', '<extra_id_99>', '<extra_id_98>', '<extra_id_97>', '<extra_id_96>', '<extra_id_95>', '<extra_id_94>', '<extra_id_93>', '<extra_id_92>', '<extra_id_91>', '<extra_id_90>', '<extra_id_89>', '<extra_id_88>', '<extra_id_87>', '<extra_id_86>', '<extra_id_85>', '<extra_id_84>', '<extra_id_83>', '<extra_id_82>', '<extra_id_81>', '<extra_id_80>', '<extra_id_79>', '<extra_id_78>', '<extra_id_77>', '<extra_id_76>', '<extra_id_75>', '<extra_id_74>', '<extra_id_73>', '<extra_id_72>', '<extra_id_71>', '<extra_id_70>', '<extra_id_69>', '<extra_id_68>', '<extra_id_67>', '<extra_id_66>', '<extra_id_65>', '<extra_id_64>', '<extra_id_63>', '<extra_id_62>', '<extra_id_61>', '<extra_id_60>', '<extra_id_59>', '<extra_id_58>', '<extra_id_57>', '<extra_id_56>', '<extra_id_55>', '<extra_id_54>', '<extra_id_53>', '<extra_id_52>', '<extra_id_51>', '<extra_id_50>', '<extra_id_49>', '<extra_id_48>', '<extra_id_47>', '<extra_id_46>', '<extra_id_45>', '<extra_id_44>', '<extra_id_43>', '<extra_id_42>', '<extra_id_41>', '<extra_id_40>', '<extra_id_39>', '<extra_id_38>', '<extra_id_37>', '<extra_id_36>', '<extra_id_35>', '<extra_id_34>', '<extra_id_33>', '<extra_id_32>', '<extra_id_31>', '<extra_id_30>', '<extra_id_29>', '<extra_id_28>', '<extra_id_27>', '<extra_id_26>', '<extra_id_25>', '<extra_id_24>', '<extra_id_23>', '<extra_id_22>', '<extra_id_21>', '<extra_id_20>', '<extra_id_19>', '<extra_id_18>', '<extra_id_17>', '<extra_id_16>', '<extra_id_15>', '<extra_id_14>', '<extra_id_13>', '<extra_id_12>', '<extra_id_11>', '<extra_id_10>', '<extra_id_9>', '<extra_id_8>', '<extra_id_7>', '<extra_id_6>', '<extra_id_5>', '<extra_id_4>', '<extra_id_3>', '<extra_id_2>', '<extra_id_1>', '<extra_id_0>']\n",
      "32099 32000\n",
      "<pad> 0\n",
      "</s> 1\n",
      "<unk> 2\n",
      "<extra_id_99> 2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2c257bcb3fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "from gluonnlp.base import get_repo_url\n",
    "from gluonnlp.data import load_vocab\n",
    "from gluonnlp.utils.misc import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['▁Hello', ',', '▁', 'y', \"'\", 'all', '!', '▁How', '▁are', '▁you', '▁VIII', '▁', '😁', '▁', '😁', '▁', '😁', '▁', '?'], ['▁', 'Glu', 'on', 'N', 'LP', '▁is', '▁great', '!', '!!!!!'], ['▁', 'Glu', 'on', 'N', 'LP', '-', 'Am', 'a', 'zon', '-', 'H', 'a', 'i', 'bin', '-', 'Le', 'on', 'ard', '-', 'She', 'ng', '-', 'Sh', 'u', 'a', 'i', '-', 'X', 'ing', 'j', 'i', 'an', '.....', '/', ':', '!', '@', '#', '▁', \"'\", 'a', 'b', 'c', \"'\"]], [[(0, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 12), (12, 13), (13, 17), (17, 21), (21, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35)], [(0, 0), (0, 3), (3, 5), (5, 6), (6, 8), (8, 11), (11, 17), (17, 18), (18, 23)], [(0, 0), (0, 3), (3, 5), (5, 6), (6, 8), (8, 9), (9, 11), (11, 12), (12, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 22), (22, 23), (23, 25), (25, 27), (27, 30), (30, 31), (31, 34), (34, 36), (36, 37), (37, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 47), (47, 48), (48, 49), (49, 51), (51, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67)]])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as dir_path: \n",
    "    model_path = os.path.join(dir_path, 't5_spm.model')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model',\n",
    "        path=model_path\n",
    "    )\n",
    "    vocab_path = os.path.join(dir_path, 't5_spm_vocab.json')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm_vocab-a9d819.json', \n",
    "        path=vocab_path\n",
    "    )\n",
    "    vocab = (json.load(vocab_path), \n",
    "    # gluon = SentencepieceTokenizer(\n",
    "    #     model_path=model_path, \n",
    "    #     vocab=load_vocab(vocab_path)\n",
    "    # )\n",
    "    # print(gluon._vocab.special_tokens)\n",
    "    # print(gluon._vocab['<extra_id_0>'], gluon._vocab['<extra_id_99>'])\n",
    "    # for token in gluon._vocab.special_tokens:\n",
    "    #     piece_id = gluon._sp_model.piece_to_id(token)\n",
    "    #     print(token, piece_id)\n",
    "    #     if gluon._sp_model.is_unknown(piece_id):\n",
    "    #         assert gluon._vocab[token] == gluon._sp_model.unk_id()\n",
    "    # os.remove(model_path)\n",
    "    # os.remove(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBWORD_TEST_SAMPLES = [\"Hello, y'all! How are you Ⅷ 😁 😁 😁 ?\",\n",
    "                        'GluonNLP is great！！！!!!',\n",
    "                        \"GluonNLP-Amazon-Haibin-Leonard-Sheng-Shuai-Xingjian...../:!@# 'abc'\"]\n",
    "\n",
    "print(gluon.encode(SUBWORD_TEST_SAMPLES, int))\n",
    "print(trans(SUBWORD_TEST_SAMPLES))\n",
    "\n",
    "print(gluon.encode_with_offsets(SUBWORD_TEST_SAMPLES))\n",
    "\n",
    "ints = gluon.encode(SUBWORD_TEST_SAMPLES, int)\n",
    "gluon.decode(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluon = SentencepieceTokenizer(\n",
    "    model_path=trans.vocab_file, \n",
    ")\n",
    "vocab = gluon._vocab.to_json()\n",
    "vocab = json.loads(vocab)\n",
    "# with open('/home/ubuntu/yongyiw/temp/test_t5spm.json', 'w') as vocab_file: \n",
    "#     json.dump(vocab, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([999,   2,   3,   4, 998,   7, 997], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def noise_span_to_unique_sentinel(tokens, noise_mask, vocab):\n",
    "  \"\"\"Replace each run of consecutive noise tokens with a different sentinel.\n",
    "  The idea here is to be able to align the dropped spans in the inputs\n",
    "  with the markers in the targets.\n",
    "  We want to generate training examples like\n",
    "  \"We hold X to be Y that\" -> \"X these truths Y self evident Z\"\n",
    "  Sentinels assigned in decreasing order within the sequence starting at\n",
    "  vocabulary.size - 1.  That is, we appropriate the last tokens in the\n",
    "  vocabulary for additional use as sentinels.\n",
    "  TODO(noam): we may want to try enlarging the vocabulary and leaving room\n",
    "  for the sentinels instead.  However, this requires enlarging the embedding\n",
    "  tables in the model, so that is a bigger change.\n",
    "  Args:\n",
    "    tokens: a 1d integer Tensor\n",
    "    noise_mask: a boolean Tensor with the same shape as tokens\n",
    "    vocabulary: a vocabulary.Vocabulary\n",
    "    seeds: an unused int32 Tensor\n",
    "  Returns:\n",
    "    a Tensor with the same shape and dtype as tokens\n",
    "  \"\"\"\n",
    "  vocab_size = vocab\n",
    "  prev_token_is_noise = tf.pad(noise_mask[:-1], [[1, 0]])\n",
    "\n",
    "  first_noise_tokens = tf.logical_and(\n",
    "      noise_mask, tf.logical_not(prev_token_is_noise))\n",
    "  subsequent_noise_tokens = tf.logical_and(noise_mask, prev_token_is_noise)\n",
    "\n",
    "  sentinel = vocab_size - tf.cumsum(tf.cast(first_noise_tokens, tokens.dtype))\n",
    "\n",
    "  tokens = tf.where(first_noise_tokens, sentinel, tokens)\n",
    "  return tf.boolean_mask(tokens, tf.logical_not(subsequent_noise_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_span_to_unique_sentinel(\n",
    "    tf.constant([1, 2, 3, 4, 5, 6, 7, 8]), \n",
    "    tf.constant([True, False, False, False, True, True, False, True]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "test() not belongs to an encoder",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-068b19eab4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-220492d5ea5b>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}() not belongs to an encoder'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: test() not belongs to an encoder"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def _assert_decoder(fn): \n",
    "    @functools.wraps(fn)\n",
    "    def wrapper(self, *args, **kwargs): \n",
    "        assert self._is_decoder, '{}() not belongs to an encoder'.format(fn.__name__)\n",
    "        return fn(self, *args, *kwargs)\n",
    "    return wrapper\n",
    "\n",
    "class A(object): \n",
    "    def __init__(self, is_decoder): \n",
    "        self._is_decoder = is_decoder\n",
    "\n",
    "    @_assert_decoder\n",
    "    def test(self): \n",
    "        print(\"Done\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enc = A(False)\n",
    "Enc.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dec = A(True)\n",
    "Dec.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.22122064  0.07740038  0.10434403  0.11839255  0.18917114]\n",
       " [-0.12347414 -0.17710291 -0.04513844  0.05793836 -0.1856082 ]\n",
       " [-0.19768797 -0.02080192  0.02444218 -0.00371607 -0.04877499]\n",
       " [-0.00226173  0.05746142  0.14661263  0.06862904  0.0354961 ]\n",
       " [ 0.10731696  0.01201746 -0.09711102 -0.07756966 -0.07882176]\n",
       " [ 0.07417728 -0.1473444  -0.10730928 -0.10424828 -0.1327885 ]\n",
       " [-0.14749663 -0.0524142   0.12662557  0.08950642 -0.06015945]\n",
       " [ 0.12040559 -0.09712193 -0.05825623  0.03717077  0.09300072]\n",
       " [-0.14225756 -0.05176199  0.20088325  0.02863085  0.05604595]\n",
       " [ 0.09697597 -0.05285374 -0.188909    0.06547912 -0.04548132]]\n",
       "<NDArray 10x5 @cpu(0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(0)\n",
    "\n",
    "layer = mx.gluon.nn.Dense(10, in_units=5, weight_initializer=mx.initializer.Normal(0.1))\n",
    "layer.initialize()\n",
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.22122064  0.07740038  0.10434403  0.11839255  0.18917114]\n",
       " [-0.12347414 -0.17710291 -0.04513844  0.05793836 -0.1856082 ]\n",
       " [-0.19768797 -0.02080192  0.02444218 -0.00371607 -0.04877499]\n",
       " [-0.00226173  0.05746142  0.14661263  0.06862904  0.0354961 ]\n",
       " [ 0.10731696  0.01201746 -0.09711102 -0.07756966 -0.07882176]\n",
       " [ 0.07417728 -0.1473444  -0.10730928 -0.10424828 -0.1327885 ]\n",
       " [-0.14749663 -0.0524142   0.12662557  0.08950642 -0.06015945]\n",
       " [ 0.12040559 -0.09712193 -0.05825623  0.03717077  0.09300072]\n",
       " [-0.14225756 -0.05176199  0.20088325  0.02863085  0.05604595]\n",
       " [ 0.09697597 -0.05285374 -0.188909    0.06547912 -0.04548132]]\n",
       "<NDArray 10x5 @cpu(0)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(0)\n",
    "\n",
    "layer = mx.gluon.nn.Dense(10, in_units=5, weight_initializer=mx.init.Normal(0.1))\n",
    "layer.initialize()\n",
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {\"V1\": 8, \"V2\": 1024, \"V3\": False}\n",
    "B = {\"V4\": 1.0, \"V5\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(V1=0, V2=0, V3=True, V4=0, V5=False): \n",
    "    print(V1, V2, V3, V4, V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "foo() got multiple values for keyword argument 'V5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-7ab1c26d1e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: foo() got multiple values for keyword argument 'V5'"
     ]
    }
   ],
   "source": [
    "foo(V5=False, **A, **B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d2e023e02a47ed911746c59eb6d179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b6dc7880a743b4a8806ac0d4483c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5 = T5Model.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'T5Model' object has no attribute 'cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-409130b16b65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/env/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'T5Model' object has no attribute 'cfg'"
     ]
    }
   ],
   "source": [
    "t5.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 2, 3, 3, 4], [3, 4, 4, 5, 5, 6], [4, 5, 5, 6, 6, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.expand_dims(A, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npx.reshape(A, (-5, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.attention_cell import (\n",
    "    gen_self_attn_mask, gen_mem_attn_mask, MultiHeadAttentionCell, RelAttentionScoreCell\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 4)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[[1, 2, 2, 2], [3, 3, 4, 3]], [[3, 4, 4, 4], [5, 5, 6, 5]], [[4, 5, 5, 5], [6, 6, 7, 6]]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = A\n",
    "mem_valid_length = np.array([1, 0, 1])\n",
    "data = np.array([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True, False]],\n",
       "\n",
       "       [[False, False]],\n",
       "\n",
       "       [[ True, False]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_mem_attn_mask(mem, mem_valid_length, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True, False]],\n",
       "\n",
       "       [[False, False]],\n",
       "\n",
       "       [[ True, False]]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_steps = npx.arange_like(mem, axis=1)  # (mem_length,)\n",
    "np.reshape(time_steps, (1, 1, -1)) < np.reshape(mem_valid_length, (-1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2, 2, 2], [3, 3, 4, 3], [3, 4, 4, 4], [5, 5, 6, 5], [4, 5, 5, 5], [6, 6, 7, 6]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1, 2, 3], [99, 98, 97]])\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.],\n",
       "       [ 1.,  2.,  3.],\n",
       "       [ 1.,  2.,  3.],\n",
       "       [ 1.,  2.,  3.],\n",
       "       [ 1.,  2.,  3.],\n",
       "       [99., 98., 97.],\n",
       "       [99., 98., 97.],\n",
       "       [99., 98., 97.],\n",
       "       [99., 98., 97.],\n",
       "       [99., 98., 97.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(B, 1).broadcast_to((2, 5, 3)).reshape(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((4 * 32, 32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "Traceback (most recent call last):\n  File \"../src/operator/numpy/np_matrix_op.cc\", line 283\nMXNetError: Check failed: d1 * d2 == static_cast<dim_t>(d0) || static_cast<dim_t>(d0) == dim_t(-1): Split dims 2, 4 do not divide original dim 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7b07f5eef96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/env/nlp/lib/python3.6/site-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, reverse, order, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/env/nlp/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mcreate_ndarray_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_np_ndarray_cls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_np_op\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/nlp/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: Traceback (most recent call last):\n  File \"../src/operator/numpy/np_matrix_op.cc\", line 283\nMXNetError: Check failed: d1 * d2 == static_cast<dim_t>(d0) || static_cast<dim_t>(d0) == dim_t(-1): Split dims 2, 4 do not divide original dim 12"
     ]
    }
   ],
   "source": [
    "B = np.zeros((12, 4))\n",
    "npx.reshape(B, (-6, 2, 4, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.zeros((3,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3., 4., 5.])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(A, np.array([0, 5, 10, 15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 2., 2.]],\n",
       "\n",
       "       [[3., 3., 4., 3.]],\n",
       "\n",
       "       [[3., 4., 4., 4.]],\n",
       "\n",
       "       [[5., 5., 6., 5.]]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(np.take(A, np.array([0, 1, 2, 3]), axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 2., 2.],\n",
       "       [3., 3., 4., 3.],\n",
       "       [3., 4., 4., 4.],\n",
       "       [5., 5., 6., 5.],\n",
       "       [4., 5., 5., 5.],\n",
       "       [6., 6., 7., 6.]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 4., 2.])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npx.pick(A, np.array([3, 1, 2, 0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(A).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': Parameter (shape=(10, 50), dtype=float32)}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nn.Embedding(10, 50)\n",
    "A.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nn.Embedding(\n",
    "    input_dim=1024, \n",
    "    output_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 512)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nn.Dense(\n",
    "    units=1024, \n",
    "    in_units=512, \n",
    "    use_bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 512)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0, 768), ctx=gpu(0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import np, npx\n",
    "\n",
    "mx.npx.set_np()\n",
    "\n",
    "A = np.zeros(shape=(1, 0, 12, 64), ctx=mx.gpu(), dtype='float32')\n",
    "npx.reshape(A, (-2, -2, -5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
