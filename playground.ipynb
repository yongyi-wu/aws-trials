{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from gluonnlp.data.tokenizers import SentencepieceTokenizer\n",
    "from gluonnlp.data.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_ids = 100\n",
    "# special_tokens = {'extra_{}_token'.format(i): '<extra_id_{}>'.format(i) for i in range(extra_ids)}\n",
    "# special_tokens['eos_token'] = '</s>'\n",
    "# special_tokens['unk_token'] = '<unk>'\n",
    "# special_tokens['pad_token'] = '<pad>'\n",
    "# gluon = SentencepieceTokenizer(\n",
    "#     model_path=trans.vocab_file,\n",
    "#     vocab=Vocab(trans.get_vocab(), **special_tokens)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792k/792k [00:00<00:00, 11.7MiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 476k/476k [00:00<00:00, 8.02MiB/s]Downloading /tmp/tmpptcvki94/t5_spm.model from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model...\n",
      "Downloading /tmp/tmpptcvki94/t5_spm_vocab.json from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm_vocab-a9d819.json...\n",
      "\n",
      "['<pad>', '</s>', '<unk>', '<extra_id_99>', '<extra_id_98>', '<extra_id_97>', '<extra_id_96>', '<extra_id_95>', '<extra_id_94>', '<extra_id_93>', '<extra_id_92>', '<extra_id_91>', '<extra_id_90>', '<extra_id_89>', '<extra_id_88>', '<extra_id_87>', '<extra_id_86>', '<extra_id_85>', '<extra_id_84>', '<extra_id_83>', '<extra_id_82>', '<extra_id_81>', '<extra_id_80>', '<extra_id_79>', '<extra_id_78>', '<extra_id_77>', '<extra_id_76>', '<extra_id_75>', '<extra_id_74>', '<extra_id_73>', '<extra_id_72>', '<extra_id_71>', '<extra_id_70>', '<extra_id_69>', '<extra_id_68>', '<extra_id_67>', '<extra_id_66>', '<extra_id_65>', '<extra_id_64>', '<extra_id_63>', '<extra_id_62>', '<extra_id_61>', '<extra_id_60>', '<extra_id_59>', '<extra_id_58>', '<extra_id_57>', '<extra_id_56>', '<extra_id_55>', '<extra_id_54>', '<extra_id_53>', '<extra_id_52>', '<extra_id_51>', '<extra_id_50>', '<extra_id_49>', '<extra_id_48>', '<extra_id_47>', '<extra_id_46>', '<extra_id_45>', '<extra_id_44>', '<extra_id_43>', '<extra_id_42>', '<extra_id_41>', '<extra_id_40>', '<extra_id_39>', '<extra_id_38>', '<extra_id_37>', '<extra_id_36>', '<extra_id_35>', '<extra_id_34>', '<extra_id_33>', '<extra_id_32>', '<extra_id_31>', '<extra_id_30>', '<extra_id_29>', '<extra_id_28>', '<extra_id_27>', '<extra_id_26>', '<extra_id_25>', '<extra_id_24>', '<extra_id_23>', '<extra_id_22>', '<extra_id_21>', '<extra_id_20>', '<extra_id_19>', '<extra_id_18>', '<extra_id_17>', '<extra_id_16>', '<extra_id_15>', '<extra_id_14>', '<extra_id_13>', '<extra_id_12>', '<extra_id_11>', '<extra_id_10>', '<extra_id_9>', '<extra_id_8>', '<extra_id_7>', '<extra_id_6>', '<extra_id_5>', '<extra_id_4>', '<extra_id_3>', '<extra_id_2>', '<extra_id_1>', '<extra_id_0>']\n",
      "32099 32000\n",
      "<pad> 0\n",
      "</s> 1\n",
      "<unk> 2\n",
      "<extra_id_99> 2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2c257bcb3fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "from gluonnlp.base import get_repo_url\n",
    "from gluonnlp.data import load_vocab\n",
    "from gluonnlp.utils.misc import download\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dir_path: \n",
    "    model_path = os.path.join(dir_path, 't5_spm.model')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm-5f05e7.model',\n",
    "        path=model_path\n",
    "    )\n",
    "    vocab_path = os.path.join(dir_path, 't5_spm_vocab.json')\n",
    "    download(\n",
    "        url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm_vocab-a9d819.json', \n",
    "        path=vocab_path\n",
    "    )\n",
    "    gluon = SentencepieceTokenizer(\n",
    "        model_path=model_path, \n",
    "        vocab=load_vocab(vocab_path)\n",
    "    )\n",
    "    print(gluon._vocab.special_tokens)\n",
    "    print(gluon._vocab['<extra_id_0>'], gluon._vocab['<extra_id_99>'])\n",
    "    for token in gluon._vocab.special_tokens:\n",
    "        piece_id = gluon._sp_model.piece_to_id(token)\n",
    "        print(token, piece_id)\n",
    "        if gluon._sp_model.is_unknown(piece_id):\n",
    "            assert gluon._vocab[token] == gluon._sp_model.unk_id()\n",
    "    os.remove(model_path)\n",
    "    os.remove(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['‚ñÅHello', ',', '‚ñÅ', 'y', \"'\", 'all', '!', '‚ñÅHow', '‚ñÅare', '‚ñÅyou', '‚ñÅVIII', '‚ñÅ', 'üòÅ', '‚ñÅ', 'üòÅ', '‚ñÅ', 'üòÅ', '‚ñÅ', '?'], ['‚ñÅ', 'Glu', 'on', 'N', 'LP', '‚ñÅis', '‚ñÅgreat', '!', '!!!!!'], ['‚ñÅ', 'Glu', 'on', 'N', 'LP', '-', 'Am', 'a', 'zon', '-', 'H', 'a', 'i', 'bin', '-', 'Le', 'on', 'ard', '-', 'She', 'ng', '-', 'Sh', 'u', 'a', 'i', '-', 'X', 'ing', 'j', 'i', 'an', '.....', '/', ':', '!', '@', '#', '‚ñÅ', \"'\", 'a', 'b', 'c', \"'\"]], [[(0, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 12), (12, 13), (13, 17), (17, 21), (21, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35)], [(0, 0), (0, 3), (3, 5), (5, 6), (6, 8), (8, 11), (11, 17), (17, 18), (18, 23)], [(0, 0), (0, 3), (3, 5), (5, 6), (6, 8), (8, 9), (9, 11), (11, 12), (12, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 22), (22, 23), (23, 25), (25, 27), (27, 30), (30, 31), (31, 34), (34, 36), (36, 37), (37, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 47), (47, 48), (48, 49), (49, 51), (51, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67)]])\n"
     ]
    }
   ],
   "source": [
    "SUBWORD_TEST_SAMPLES = [\"Hello, y'all! How are you ‚Öß üòÅ üòÅ üòÅ ?\",\n",
    "                        'GluonNLP is greatÔºÅÔºÅÔºÅ!!!',\n",
    "                        \"GluonNLP-Amazon-Haibin-Leonard-Sheng-Shuai-Xingjian...../:!@# 'abc'\"]\n",
    "\n",
    "print(gluon.encode(SUBWORD_TEST_SAMPLES, int))\n",
    "print(trans(SUBWORD_TEST_SAMPLES))\n",
    "\n",
    "print(gluon.encode_with_offsets(SUBWORD_TEST_SAMPLES))\n",
    "\n",
    "ints = gluon.encode(SUBWORD_TEST_SAMPLES, int)\n",
    "gluon.decode(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.base import get_repo_url\n",
    "from gluonnlp.data import Vocab, load_vocab\n",
    "from gluonnlp.utils.misc import download\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 540k/540k [00:00<00:00, 7.96MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ubuntu/yongyiw/temp/t5_spm.json from s3://gluonnlp-numpy-data/tokenizer_test_models/sentencepiece/case_t5/test_t5spm-821614.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/yongyiw/temp/t5_spm.json'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_path = os.path.join('/home/ubuntu/yongyiw/temp', 't5_spm.json')\n",
    "download(\n",
    "    url=get_repo_url() + 'tokenizer_test_models/sentencepiece/case_t5/test_t5spm-821614.json', \n",
    "    path=vocab_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1dfab7b16b65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/yongyiw/models/gluon-nlp/src/gluonnlp/data/vocab.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         raise NotImplementedError('Type of the input vocab is not supported. '\n",
      "\u001b[0;32m~/yongyiw/models/gluon-nlp/src/gluonnlp/data/vocab.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yongyiw/models/gluon-nlp/src/gluonnlp/data/vocab.py\u001b[0m in \u001b[0;36mfrom_json\u001b[0;34m(cls, json_str)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \"\"\"\n\u001b[1;32m    416\u001b[0m         \u001b[0mvocab_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mspecial_token_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'special_token_key_value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'unk_token'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecial_token_kv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "v = load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = gluon._vocab.to_json()\n",
    "vocab = json.loads(vocab)\n",
    "with open('/home/ubuntu/yongyiw/temp/test_t5spm.json', 'w') as vocab_file: \n",
    "    json.dump(vocab, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_span_to_unique_sentinel(tokens, noise_mask, vocab):\n",
    "  \"\"\"Replace each run of consecutive noise tokens with a different sentinel.\n",
    "  The idea here is to be able to align the dropped spans in the inputs\n",
    "  with the markers in the targets.\n",
    "  We want to generate training examples like\n",
    "  \"We hold X to be Y that\" -> \"X these truths Y self evident Z\"\n",
    "  Sentinels assigned in decreasing order within the sequence starting at\n",
    "  vocabulary.size - 1.  That is, we appropriate the last tokens in the\n",
    "  vocabulary for additional use as sentinels.\n",
    "  TODO(noam): we may want to try enlarging the vocabulary and leaving room\n",
    "  for the sentinels instead.  However, this requires enlarging the embedding\n",
    "  tables in the model, so that is a bigger change.\n",
    "  Args:\n",
    "    tokens: a 1d integer Tensor\n",
    "    noise_mask: a boolean Tensor with the same shape as tokens\n",
    "    vocabulary: a vocabulary.Vocabulary\n",
    "    seeds: an unused int32 Tensor\n",
    "  Returns:\n",
    "    a Tensor with the same shape and dtype as tokens\n",
    "  \"\"\"\n",
    "  vocab_size = vocab\n",
    "  prev_token_is_noise = tf.pad(noise_mask[:-1], [[1, 0]])\n",
    "\n",
    "  first_noise_tokens = tf.logical_and(\n",
    "      noise_mask, tf.logical_not(prev_token_is_noise))\n",
    "  subsequent_noise_tokens = tf.logical_and(noise_mask, prev_token_is_noise)\n",
    "\n",
    "  sentinel = vocab_size - tf.cumsum(tf.cast(first_noise_tokens, tokens.dtype))\n",
    "\n",
    "  tokens = tf.where(first_noise_tokens, sentinel, tokens)\n",
    "  return tf.boolean_mask(tokens, tf.logical_not(subsequent_noise_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([999,   2,   3,   4, 998,   7, 997], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_span_to_unique_sentinel(\n",
    "    tf.constant([1, 2, 3, 4, 5, 6, 7, 8]), \n",
    "    tf.constant([True, False, False, False, True, True, False, True]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7d749d5afefb4ad7260c6ef01b357a620f7fe60d36d436fd76526fc77be9242c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}